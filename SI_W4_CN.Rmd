---
title: "Week 4 Classnotes"
author: "Yigit Ozan Berk"
date: "6/21/2019"
output: html_document
---

# Power = 1 - Type II error rate

We've talked about a Type I error, rejecting the null hypothesis when it's true. We've structured our hypothesis test so that the probability of this happening is small. The other kind of error we could make is to fail to reject when the alternative is true (Type II error). Or we might think about the probability of rejecting the null when it is false. This is called Power = 1 - Type II error. We don't have as much control over this probability, since we've spent all of our flexibility guaranteeing that the Type I error rate is small.

One avenue for the control of power is at the design phase. There, assuming our finances let us, we can pick a large enough sample size so that we'd be likely to reject if the alternative is true. Thus the most frequent use of power is to help us design studies.


# Contents

- Power
- Multipe Comparisons
- Resampling
- Quiz
- Course Project
- Swirl Excercises

# Power

power is the probability of rejecting the null hypothesis when it is false.

```{r}
z <- qnorm(1 - alpha)
pnorm(mu0 + z * sigma/sqrt(n), mean = mua, sd = sigma/sqrt(n), lower.tail = FALSE)
```


As you collect more and more sample for data, the power increases faster.

```{r}
library(manipulate)
library(ggplot2)
mu0 <- 30
#alpha is Type I error rate
myplot <- function(sigma, mua, n, alpha) {
        g <- ggplot(data.frame(mu = c(27, 36)), aes(x = mu))
        g <- g + stat_function(fun= dnorm, geom = "line",
                               args = list(mean = mu0, sd = sigma/sqrt(n)),
                               size = 2, col = "red")
        g <- g + stat_function(fun = dnorm, geom  = "line",
                               args = list(mean = mua, sd = sigma/sqrt(n)),
                               size = 2, col = "blue")
        xitc = mu0 + qnorm(1- alpha)*sigma/sqrt(n)
        g <- g + geom_vline(xintercept = xitc, size = 3)
        g
}

manipulate(
        myplot(sigma, mua, n, alpha),
        sigma = slider(1, 10, step = 1, initial = 4),
        mua = slider(30, 35, step = 1, initial = 32),
        n = slider(1, 50, step = 1, initial = 16),
        alpha = slider(0.01, 0.1, step = 0.01, initial = 0.05)
)

```

- power goes up as alpha gets larger
- power of a one sided test is greater than the power of the associated two sided test(alpha/2)
- power goes up as mu1 gets further away from mu0
- power goes up as n goes up

# T-test power

delta equals mua - mu0
delta = mua - mu0

```{r}
power.t.test(n = 16, delta = 2/4, sd = 1, type = "one.sample", alt = "one.sided")$power

```

```{r}
power.t.test(n = 16, delta = 2, sd = 4, type = "one.sample", alt = "one.sided")$power
```

```{r}
power.t.test(n = 16, delta = 100, sd = 200, type = "one.sample", alt = "one.sided")$power
```

because we keep delta/sd same, the output is equivalent

let's try to calculate sample size

```{r}
power.t.test(power = 0.8, delta = 2/4, sd = 1, type = "one.sample", alt = "one.sided")$n
```

```{r}
power.t.test(power = 0.8, delta = 2, sd = 4, type = "one.sample", alt = "one.sided")$n
```

```{r}
power.t.test(power = 0.8, delta = 100, sd = 200, type = "one.sample", alt = "one.sided")$n
```


# Multiple Testing

when you're doing more than 1 hypothesis tests, you need to check your results for correction.

Hypothesis testing/significance analysis is commonly overused

correcting for multiple testing avoids false positives or discoveries

two key components:

- error measure
- correction


Error rates to check in a multiple testing environment :

- false positive rate : the rate at which false results (beta = 0) are called significant  E[V/m0]
- family wise error rate (FWER) : the probability of at least one false positive
Pr (V >= 1)
- false discovery rate(FDR) : the rate at which claims of significance are false E[V/R]
(meaning of letters in the screenshot of the table)


ANOTHER Approach:

Adjusted P-Values

- one approach is to adjust the threshold alpha
- a different aproach is to calculate "adjusted p-values"
- They are not p-values anymore!!!!
- but they can be used directly without adjusting alpha

```{r}
set.seed(1010093)
pValues <- rep(NA, 1000)
for(i in 1:1000) {
        y <- rnorm(20)
        x <- rnorm(20)
        pValues[i] <- summary(lm(y ~ x))$coeff[2,4]
        #2nd row 4th column is the p-value for the relationship between y and x
}

sum(pValues <0.05)
```

Even there is no connection between y and x, still we get 5 % of tests being performed are called significant

```{r}
#controls FWER
sum(p.adjust(pValues, method = "bonferroni") < 0.05)
```

```{r}
#Controls FDR
sum(p.adjust(pValues, method = "BH") <0.05)
#benjamini hawkbird correction for controlling FDR

```

Case study II : 50% true positives

```{r}
set.seed(1010093)
pValues <- rep(NA, 1000)

for(i in 1:1000){
        x <- rnorm(20)
        #first 500 beta = 0, last 500 beta = 2
        if(i <= 500){y <- rnorm(20)}else{y <- rnorm(20, mean= 2 * x)}
        pValues[i] <- summary(lm(y~x))$coeff[2, 4]
}
trueStatus <- rep(c("zero", "non zero"), each = 500)
table(pValues < 0.05, trueStatus)
```

There are %5 false positives

```{r}
# Controls FWER
table(p.adjust(pValues, method = "bonferroni") < 0.05, trueStatus)
```

we get rid of false positives but we also lose true positives.

```{r}
# Controls FDR
table(p.adjust(pValues, method = "BH") <0.05, trueStatus)
```

fewer false positives without losing true positives.


p-values versus adjusted p-values to understand what p adjustment does
```{r}
par(mfrow = c(1,2))
plot(pValues, p.adjust(pValues, method = "bonferroni"), pch = 19)
plot(pValues, p.adjust(pValues, method = "BH"), pch = 19)
```

# Resampling

- Bootstrapping
- Notes on bootsrapping
- permutation

a tremendously useful tool for constructing confidence intervals and calculating standard errors for difficult statistics

one of the most important procedures discovered. really liberated data analysts for inferences.

for example, how would one derive a confidence interval for the median?

# bootstrapping example

```{r}
library(UsingR)
data(father.son)
x <- father.son$sheight
n <- length(x)
B <- 10000
resamples <- matrix(sample(x, n * B, replace = TRUE), B, n)
# draw from x(sample()) use n * B numbers of resamples, where every time you take an observation you can replicate.
# arrange with Bootstrap rows, number of sample sizes columns
#arrange for B rows, n columns
# every row of this matrix is now a completely resampled data set  with original datas and sample size from the original data set.
resampleMedians <- apply(resamples, 1, median)
qplot(resampleMedians)
```

# Notes on Bootstrap

Principle : if you have a statistic that estimates some population parameter, but I don't know its sampling distribution

the bootstrap principle suggests using the distribution defined by the data to approximate sampling distribution

in practice, the bootstrap principle is always carried out using simulation

we will cover only a few aspects of bootstrap resampling

1 - sample n observations with replacement from the observed data resulting in one simulated complete data set.

2 - take the median of the simulated data set

3 - repeat these two steps B times, resulting in B simulated medians.(10000 or more simulations)

you want B to be large for the Monte Carlo error to be small.

4 - these medians are approximately drawn from the sampling distribution of the median of n observations; therefore we can
- draw a histogram of them
- calculate their standard deviation to estimate the standard error of the median
- take the 2.5% and 97.5 % percentiles as a confidence interval for the median

```{r}
B <- 10000
medians <- apply(resamples, 1, median)
sd(medians)
```

estimated standard error of the median

```{r}
quantile(medians, c(0.025, 0.975))
```

fairly tight confidence interval

```{r}
g <- ggplot(data.frame(medians = medians), aes(x = medians))
g <- g + geom_histogram(color = "black", fill = "lightblue", binwidth = 0.05)
g
```

this plot is an ESTIMATE of the sample distribution of the median

if we had the true population distribution, and sample it over and over again, this would be a good estimate with Monte Carlo error.


- bootstrap is non-parametric
- better percentile bootstrap confidence intervals correct for bias
- there are lots of variations on bootstrap procedures; the book "An introduction to the Bootstrap" by Efron and Tibshirani is a great place to start for both bootstrap and jackknife information

# permutation tests

used for group comparisons

- consider the null hypothesis that the distribution of the observations from each group is the same
- then, the group labels are irrelevant
- consider a data fram with count column and spray(group) label column
- permute the spray(group) labels column
- recalculate the statistics
mean difference in counts, geometric means, T statistic..
- calculate the percentage of simulations where the simulated statistic was more extreme (toward the alternative) than the observed


```{r}
subdata <- InsectSprays[InsectSprays$spray %in% c("B", "C"), ]
y <- subdata$count
group <- as.character(subdata$spray)
testStat <- function(w, g) mean(w[g == "B"]) - mean(w[g == "C"])
observedStat <- testStat(y, group)
#correct data
permutations <- sapply(1: 10000, function(i) testStat(y, sample(group)))
#sampled data
#under the null hypothesis that the group label is not related to outcome.
observedStat

```

```{r}
mean(permutations > observedStat)
```

in this data set we get 0. we couldn't find a configuration

the p value is very small, close to zero(scientificly, because the permutation form of our original data is at least as big as observedStat.). so we of course reject the null hypothesis.

```{r}
qplot(permutations) + geom_vline(xintercept = 13.25)
```

as the plot shows, the original difference in original data are on the far right, as we guessed before. we can now formally present a null hypothesis test.


# swirl exc

- power
- multiple testing
- resampling

# power

| Power comes into play when you're designing an experiment, and in
| particular, if you're trying to determine if a null result (failing to
| reject a null hypothesis) is meaningful. For instance, you might have to
| determine if your sample size was big enough to yield a meaningful, rather
| than random, result.

...

  |===                                                                 |   4%
| Power gives you the opportunity to detect if your ALTERNATIVE hypothesis is
| true.

| Beta is the probability of a Type II error, accepting a false null
| hypothesis; the complement of this is obviously (1 - beta) which represents
| the probability of rejecting a false null hypothesis. This is good and this
| is POWER!

| Suppose we're testing a null hypothesis H_0 with an alpha level of .05.
| Since H_a proposes that mu > 30 (the mean hypothesized by H_0), power is
| the probability that the true mean mu is greater than the (1-alpha)
| quantile or qnorm(.95). For simplicity, assume we're working with normal
| distributions of which we know the variances.



| First we have to emphasize a key point. The two hypotheses, H_0 and H_a,
| actually represent two distributions since they're talking about means or
| centers of distributions. H_0 says that the mean is mu_0 (30 in our
| example) and H_a says that the mean is mu_a.

...

  |==========                                                          |  15%
| We're assuming normality and equal variance, say sigma^2/n, for both
| hypotheses, so under H_0, X'~ N(mu_0, sigma^2/n) and under H_a, X'~ N(mu_a,
| sigma^2/n).




| Now back to numbers. Our test for determining rejection of H_0 involved
| comparing a test statistic, namely Z=(X'-30)/(sigma/sqrt(n)), against some
| quantile, say Z_95, which depended on our level size alpha (.05 in this
| case). H_a proposed that mu > mu_0, so we tested if Z>Z_95.  This is
| equivalent to X' > Z_95 * (sigma/sqrt(n)) + 30, right?

...

  |===========================                                         |  40%
| Recall that nifty R function pnorm, which gives us the probability that a
| value drawn from a normal distribution is greater or less than/equal to a
| specified quantile argument depending on the flag lower.tail. The function
| also takes a mean and standard deviation as arguments.


| Suppose we call pnorm with the quantile 30 + Z_95 * (sigma/sqrt(n)) and
| specify mu_a as our mean argument. This would return a probability which we
| can interpret as POWER. Why?


 Recall our picture of two distributions. 30 + Z_95 * (sigma/sqrt(n))
| represents the point at which our vertical line falls. It's the point on
| the null distribution at the (1-alpha) level.


 Study this picture. Calling pnorm with 30 + Z_95 * (sigma/sqrt(n)) as the
| quantile and mu_a, say 32, as the mean and lower.tail=FALSE does what?

returns the area under the blue curve to the right of the line




First, define a variable z as qnorm(.95)

> z <- qnorm(.95)

| Keep up the great work!

  |================================                                    |  47%
| Run pnorm now with the quantile 30+z, mean=30, and lower.tail=FALSE. We've
| specified sigma and n so that the standard deviation of the sample mean is
| 1.

> pnorm(30 + z, mean = 30, lower.tail = FALSE)
[1] 0.05

| Keep up the great work!

  |=================================                                   |  48%
| That's not surprising, is it? With the mean set to mu_0 the two
| distributions, null and alternative, are the same and power=alpha. Now run
| pnorm now with the quantile 30+z, mean=32, and lower.tail=FALSE.

> pnorm(30+z, mean = 32, lower.tail = FALSE)
[1] 0.63876

| Keep working like that and you'll get there!

  |=================================                                   |  49%
| See how this is much more powerful? 64% as opposed to 5%. When the sample
| mean is quite different from (many standard errors greater than) the mean
| hypothesized by the null hypothesis, the probability of rejecting H_0 when
| it is false is much higher. That is power!

To see this, run pnorm now with the quantile 30+z, mean=32 and sd=1. Don't
| forget to set lower.tail=FALSE so you get the right tail.

> pnorm(30 + z, mean = 32, sd = 1, lower.tail = FALSE)
[1] 0.63876

| That's correct!

  |====================================                                |  53%
| Now run pnorm now with the quantile 30+z*2, mean=32 and sd=2. Don't forget
| to set lower.tail=FALSE so you get the right tail.

> pnorm(30 + z, mean = 32, sd = 2, lower.tail = FALSE)
[1] 0.5704709

| You almost had it, but not quite. Try again. Or, type info() for more
| options.

| Type pnorm(30+z*2,mean=32,sd=2,lower.tail=FALSE) at the command prompt.

> pnorm(30 + z*2, mean = 32, sd = 2, lower.tail = FALSE)
[1] 0.259511

| Excellent job!

  |=====================================                               |  54%
| See the power drain from 64% to 26% ? Let's review some basic facts about
| power. We saw before in our pictures that the power of the test depends on
| mu_a. When H_a specifies that mu > mu_0, then as mu_a grows and exceeds
| mu_0 increasingly, what happens to power?

increases



 If H_a proposed that mu != mu_0 we would calculate the one sided power
| using alpha / 2 in the direction of mu_a (either less than or greater than
| mu_0). (This is only approximately right, it excludes the probability of
| getting a large test statistic in the opposite direction of the truth.

Finally, if H_a specified that mu < mu_0 could we still do the same kind of
| power calculations?

1: Yes
2: No

Selection: 1

| You got it!

  |============================================                        |  65%
| Suppose H_a says that mu > mu_0. Then power = 1 - beta = Prob ( X' > mu_0 +
| z_(1-alpha) * sigma/sqrt(n)) assuming that X'~ N(mu_a,sigma^2/n). Which
| quantities do we know in this statement, given the context of the problem?
| Let's work through this.


 After the null mean mu_0 is proposed what does the designer of the
| hypothesis test specify in order to reject or fail-to-reject H_0? In other
| words, what is the level size of the test?

1: mu_a
2: alpha
3: mu_0
4: beta

Selection: 2

| You got it!

  |===============================================                     |  68%
| So we know that the quantities mu_0 and alpha are specified by the test
| designer. In the statement 1 - beta = Prob( X' > mu_0 + z_(1-alpha) *
| sigma/sqrt(n)) given mu_a > mu_0, mu_0 and alpha are specified, and X'
| depends on the data. The other four quantities, (beta, sigma, n, and mu_a),
| are all unknown.

...

  |===============================================                     |  70%
| It should be obvious that specifying any three of these unknowns will allow
| us to solve for the missing fourth. Usually, you only try to solve for
| power (1-beta) or the sample size n.

...

  |================================================                    |  71%
| An interesting point is that power doesn't need mu_a, sigma and n
| individually.  Instead only sqrt(n)*(mu_a - mu_0) /sigma is needed. The
| quantity (mu_a - mu_0) / sigma is called the EFFECT SIZE. This is the
| difference in the means in standard deviation units. It is unit free so it
| can be interpreted in different settings.


We'll work through some examples of this now. However, instead of assuming
| that we're working with normal distributions let's work with t
| distributions. Remember, they're pretty close to normal with large enough
| sample sizes.

...

  |==================================================                  |  73%
| Power is still a probability, namely P( (X' - mu_0)/(S /sqrt(n)) >
| t_(1-alpha, n-1) given H_a that mu > mu_a ). Notice we use the t quantile
| instead of the z. Also, since the proposed distribution is not centered at
| mu_0, we have to use the non-central t distribution.

...

  |==================================================                  |  74%
| R comes to the rescue again with the function power.t.test. We can omit one
| of the arguments and the function solves for it. Let's first use it to
| solve for power.


We'll run it three times with the same values for n (16) and alpha (.05)
| but different delta and standard deviation values. We'll show that if delta
| (difference in means) divided by the standard deviation is the same, the
| power returned will also be the same. In other words, the effect size is
| constant for all three of our tests.



# multiple testing

| Given that data is valuable and we'd like to get the most out of it, we
| might use it to test several hypotheses. If we have an alpha level of .05
| and we test 20 hypotheses, then on average, we expect one error, just by
| chance.

 Another potential problem is that after running several tests, only the
| lowest p-value might be reported OR all p-values under some threshold might
| be considered significant. Undoubtedly, some of these would be false.

Luckily, we have clever ways of minimizing errors in this situation. That's
| what we'll address.  We'll define specific error measures and then
| statistical ways of correcting or limiting them.

Multiple testing is particularly relevant now in this age of BIG data.
| Statisticians are tasked with questions such as "Which variables matter
| among the thousands measured?" and "How do you relate unrelated
| information?"


The p-value is "the probability under the null hypothesis of obtaining
| evidence as or more extreme than your test statistic (obtained from your
| observed data) in the direction of the alternative hypothesis." Of course
| p-values are related to significance or alpha levels, which are set before
| the test is conducted (often at 0.05).


If a p-value is found to be less than alpha (say 0.05), then the test
| result is considered statistically significant, i.e., surprising and
| unusual, and the null hypothesis (the status quo) is ?

1: accepted
2: rejected
3: revised
4: renamed the aleph null hypothesis

Selection: 
Enter an item from the menu, or 0 to exit
Selection: 2

| Keep up the great work!

  |==============                                                      |  21%
| Now consider this chart copied from
| http://en.wikipedia.org/wiki/Familywise_error_rate. Suppose we've tested m
| null hypotheses, m_0 of which are actually true, and m-m_0 are actually
| false. Out of the m tests R have been declared significant, that is, the
| associated p-values were less than alpha, and m-R were nonsignificant, or
| boring results.

...

  |================                                                    |  23%
| Looking at the chart, which variables are known?

1: A,B,C
2: m and R
3: S,T,U,V
4: m_0, and m

Selection: 2

| Keep working like that and you'll get there!

  |=================                                                   |  25%
| In testing the m_0 true null hypotheses, V results were declared
| significant, that is, these tests favored the alternative
| hypothesis. What type of error does this represent?

1: Type I
2: a serious one
3: Type II
4: Type III

Selection: 1

| That's a job well done!

  |==================                                                  |  26%
| Another name for a Type I error is False Positive, since it is
| falsely claiming a significant (positive) result.

...

  |===================                                                 |  28%
| Of the m-m_0 false null hypotheses, T were declared nonsignificant.
| This means that these T null hypotheses were accepted (failed to be
| rejected). What type of error does this represent?

1: Type II
2: Type I
3: a serious one
4: Type III

Selection: 1

| You are really on a roll!

  |====================                                                |  30%
| Another name for a Type II error is False Negative, since it is
| falsely claiming a nonsignificant (negative) result.

The observed R represents the number of test results declared
| significant. These are 'discoveries', something different from the
| status quo. V is the number of those falsely declared significant,
| so V/R is the ratio of FALSE discoveries. Since V is a random
| variable (i.e., unknown until we do an experiment) we call the
| expected value of the ratio, E(V/R), the False Discovery Rate (FDR).

A rose by any other name, right? How about the fraction V/m_0? From
| the chart, m_0 represents the number of true H_0's and m_0 is
| unknown. V is the number of those falsely declared significant, so
| V/m_0 is the ratio of FALSE positives. Since V is a random variable
| (i.e., unknown until we do an experiment) we call the expected value
| of the ratio, E(V/m_0), the FALSE POSITIVE rate.



Suppose we're really smart, calculate our p-values correctly, and
| declare all tests with p < alpha as significant. This means that our
| false positive rate is at most alpha, on average.

Suppose we perform 10,000 tests and alpha = .05. How many false
| positives do we expect on average?

1: 50000
2: 500
3: 5000
4: 50

Selection: 2

| You are quite good my friend!

  |===============================                                     |  46%
| You got it! 500 false positives seems like a lot. How do we avoid so
| many?

| It's very straightforward. We do m tests and want to control the
| FWER at level alpha so that Pr(V >= 1) < alpha. We simply reduce
| alpha dramatically. Set alpha_fwer to be alpha/m. We'll only call a
| test result significant if its p-value < alpha_fwer.

Sounds good, right? Easy to calculate. What would be a drawback with
| this method?

1: too many results will pass
2: too many results will fail
3: requires too much math

Selection: 2

| Excellent work!

  |====================================                                |  52%
| Another way to limit the false positive rate is to control the false
| discovery rate (FDR). Recall this is E(V/R). This is the most
| popular correction when performing lots of tests. It's used in lots
| of areas such as genomics, imaging, astronomy, and other
| signal-processing disciplines.

Again, we'll do m tests but now we'll set the FDR, or E(V/R) at
| level alpha. We'll calculate the p-values as usual and order them
| from smallest to largest, p_1, p_2,...p_m. We'll call significant
| any result with p_i <= (alpha*i)/m. This is the Benjamini-Hochberg
| method (BH). A p-value is compared to a value that depends on its
| ranking.

Like the Bonferroni correction, this is easy to calculate and it's
| much less conservative. It might let more false positives through
| and it may behave strangely if the tests aren't independent.

...

  |========================================                            |  59%
| Now consider this chart copied from the slides. It shows the
| p-values for 10 tests performed at the alpha=.2 level and three
| cutoff lines. The p-values are shown in order from left to right
| along the x-axis. The red line is the threshold for No Corrections
| (p-values are compared to alpha=.2), the blue line is the Bonferroni
| threshold, alpha=.2/10 = .02, and the gray line shows the BH
| correction. Note that it is not horizontal but has a positive slope
| as we expect.

With no correction, how many results are declared significant?

1: 2
2: 4
3: 8
4: 6

Selection: 2

| You nailed it! Good job!

  |==========================================                          |  62%
| With the Bonferroni correction, how many tests are declared
| significant?

1: 6
2: 8
3: 2
4: 4

Selection: 3

| You're the best!

  |===========================================                         |  64%
| So the Bonferroni passed only half the results that the No
| Correction (comparing p-values to alpha) method passed. Now look at
| the BH correction. How many tests are significant with this scale?

1: 3
2: 5
3: 7
4: 1

Selection: 1

| Keep up the great work!

  |=============================================                       |  66%
| So the BH correction which limits the FWER is between the No
| Correction and the Bonferroni. It's more conservative (fewer
| significant results) than the No Correction but less conservative
| (more significant results) than the Bonferroni. Note that with this
| method the threshold is proportional to the ranking of the values so
| it slopes positively while the other two thresholds are flat.


Notice how both the Bonferroni and BH methods adjusted the threshold
| (alpha) level of rejecting the null hypotheses. Another equivalent
| corrective approach is to adjust the p-values, so they're not
| classical p-values anymore, but they can be compared directly to the
| original alpha.



Suppose the p-values are p_1, ... , p_m.  With the Bonferroni method
| you would adjust these by setting p'_i = max(m * p_i, 1) for each
| p-value. Then if you call all p'_i < alpha significant you will
| control the FWER.

...

  |================================================                    |  70%
| To demonstrate some of these concepts, we've created an array of
| p-values for you. It is 1000-long and the result of a linear
| regression performed on random normal x,y pairs so there is no true
| significant relationship between the x's and y's.


Use the R command head to see the first few entries of the array
| pValues.

> head(pValues)
[1] 0.5334915 0.2765785 0.8380943 0.6721730 0.8122037 0.4078675

| Great job!

  |==================================================                  |  74%
| Now count the number of entries in the array that are less than the
| value .05. Use the R command sum, and the appropriate Boolean
| expression.

> sum(pValues < .05)
[1] 51

| Your dedication is inspiring!

  |===================================================                 |  75%
| So we got around 50 false positives, just as we expected
| (.05*1000=50). The beauty of R is that it provides a lot of built-in
| statistical functionality. The function p.adjust is one example. The
| first argument is the array of pValues. Another argument is the
| method of adjustment. Once again, use the R function sum and a
| boolean expression using p.adjust with method="bonferroni" to
| control the FWER.


sum(p.adjust(pValues, method = "bonferroni") < .05)
[1] 0

| You are doing so well!

  |====================================================                |  77%
| So the correction eliminated all the false positives that had passed
| the uncorrected alpha test. Repeat the same experiment, this time
| using the method "BH" to control the FDR.

sum(p.adjust(pValues, method = "BH") < .05)
[1] 0

| All that hard work is paying off!

  |======================================================              |  79%
| So the BH method also eliminated all the false positives. Now we've
| generated another 1000-long array of p-values, this one called
| pValues2. In this data, the first half ( 500 x/y pairs) contains x
| and y values that are random and the second half contain x and y
| pairs that are related, so running a linear regression model on the
| 1000 pairs should find some significant (not random) relationship.

We also created a 1000-long array of character strings, trueStatus.
| The first 500 entries are "zero" and the last are "not zero". Use
| the R function tail to look at the end of trueStatus.

> tail(trueStatus)
[1] "not zero" "not zero" "not zero" "not zero" "not zero" "not zero"

| Great job!

  |========================================================            |  82%
| Once again we can use R's greatness to count and tabulate for us. We
| can call the R function table with two arguments, a boolean such as
| pValues2<.05, and the array trueStatus. The boolean obviously has
| two outcomes and each entry of trueStatus has one of two possible
| values. The function table aligns the two arguments and counts how
| many of each combination (TRUE,"zero"), (TRUE,"not zero"),
| (FALSE,"zero"), and (FALSE,"not zero") appear. Try it now.

> table(pValues2 <.05, trueStatus)
       trueStatus
        not zero zero
  FALSE        0  476
  TRUE       500   24

| Your dedication is inspiring!

  |=========================================================           |  84%
| We see that without any correction all 500 of the truly significant
| (nonrandom) tests were correctly identified in the "not zero"
| column. In the zero column (the truly random tests), however, 24
| results were flagged as significant.


What is the percentage of false positives in this test?

> 24/500
[1] 0.048

| You are doing so well!

  |===========================================================         |  87%
| Just as we expected - around 5% or .05*100.

...

  |============================================================        |  89%
| Now run the same table function, however, this time use the call to
| p.adjust with the "bonferroni" method in the boolean expression.
| This will control the FWER.

table(p.adjust(pValues2, method = "bonferroni") <.05, trueStatus)
       trueStatus
        not zero zero
  FALSE       23  500
  TRUE       477    0

| All that hard work is paying off!

  |=============================================================       |  90%
| Since the Bonferroni correction method is more conservative than
| just comparing p-values to alpha all the truly random tests are
| correctly identified in the zero column. In other words, we have no
| false positives. However, the threshold has been adjusted so much
| that 23 of the truly significant results have been misidentified in
| the not zero column.

Now run the same table function one final time. Use the call to
| p.adjust with "BH" method in the boolean expression. This will
| control the false discovery rate.

> table(p.adjust(pValues2, method = "BH") <.05, trueStatus)
       trueStatus
        not zero zero
  FALSE        0  487
  TRUE       500   13

| Your dedication is inspiring!

  |================================================================    |  93%
| Again, the results are a compromise between the No Corrections and
| the Bonferroni. All the significant results were correctly
| identified in the "not zero" column but in the random ("zero")
| column 13 results were incorrectly identified. These are the false
| positives. This is roughly half the number of errors in the other
| two runs.


Here's a plot of the two sets of adjusted p-values, Bonferroni on
| the left and BH on the right. The x-axis indicates the original
| p-values. For the Bonferroni, (adjusting by multiplying by 1000, the
| number of tests), only a few of the adjusted values are below 1. For
| the BH, the adjusted values are slightly larger than the original
| values.


We'll conclude by saying that multiple testing is an entire subfield
| of statistical inference. Usually a basic Bonferroni/BH correction
| is good enough to eliminate false positives, but if there is strong
| dependence between tests there may be problems. Another correction
| method to consider is "BY".

...

  |=================================================================== |  98%
| Congrats! We hope you liked the multiple concepts and questions you
| saw in this lesson.





# resampling

| In this lesson, you get a bonus! We'll talk about two topics in
| statistical inference, bootstrapping AND permutation testing. These
| both fall under the broader category of resampling methods. We'll
| start with bootstrapping.


| The bootstrap is a handy tool for making statistical inferences. It
| is used in constructing confidence intervals and calculating
| standard errors for statistics that might be difficult for some
| reason (e.g., lack of data or no closed form). Wikipedia tells us
| that bootstrapping is a technique which "allows estimation of the
| sampling distribution of almost any statistic using very simple
| methods."  Simple is good, right?


 The beauty of bootstrapping is that it avoids complicated
| mathematics and instead uses simulation and computation to infer
| distributional properties you might not otherwise be able to
| determine.

...

  |===                                                          |   6%
| It's relatively new, developed in 1979, by Bradley Efron, a Stanford
| statistician.  The basic bootstrap principle uses OBSERVED data to
| construct an ESTIMATED population distribution using random sampling
| with replacement. From this distribution (constructed from the
| observed data) we can estimate the distribution of the statistic
| we're interested in.


So, in bootstrapping the observed data substitutes for what?

1: a population
2: a statistic
3: observations
4: a hypothesis

Selection: 1

| That's the answer I was looking for.

  |=====                                                        |   8%
| So, in bootstrapping if the observed data is the population, what
| would the random samplings correspond to?

1: a population
2: a hypothesis
3: a statistic
4: observations

Selection: 4

| You're the best!

  |======                                                       |  10%
| In effect, the original observed sample substitutes for the
| population. Our samplings become observations from which we estimate
| a statistic and get an idea about its distribution. This lets us
| better understand the underlying population (from which we didn't
| have enough data).

 Here's a critical point. In constructing the estimated distribution
| we sample the observed data WITH replacement. If the original sample
| is n long and we sampled n times without replacement what would we
| get?

1: a better sample
2: an entirely new sample
3: the original sample permuted
4: a worse sample

Selection: 3

| Excellent work!

  |========                                                     |  12%
| The motivating example from the slides involves computing the
| average of 50 rolls of a die. Of course we can do this theoretically
| when we know that the die is fair. Remember, E(x) = Sum(x*p(x)) for
| x=1,2,...6, and p(x)=1/6 for all values of x.

For the heck of it, compute the expected die roll for a fair die.

> mean(c(1, 2, 3, 4, 5, 6))
[1] 3.5

| That's the answer I was looking for.

  |=========                                                    |  15%
| Theoretically, the average is 3.5. Here, we've run code and plotted
| a histogram after we took 1000 such averages, each of 50 dice rolls.
| Note the unusual y-axis scale. We're displaying this as a density
| function so the area of the salmon-colored region is theoretically
| 1. With this scale, though, all the heights of the bins actually add
| up to 5. So you have to multiply each height by .2 and add up all
| the results to get 1.


What if some joker wanted you to run the same experiment with a die
| he gave you and he warned you that the dice was loaded? In other
| words, it wasn't fair. It has some random distribution like this.

...

  |============                                                 |  19%
| The outcomes aren't equally likely, are they? So when you do your
| 1000 runs of 50 rolls each, the density of the means looks
| different.


We've done this for you and put the result in g2. Type print(g2) now
| to see the picture.

> print(g2)

| All that hard work is paying off!

  |==============                                               |  22%
| Picture's a little different, right? Although this example is a bit
| contrived, it illustrates an important concept. We really want a
| distribution of means and we have only one set of observations. (In
| this case it was the empirical distribution associated with the
| unfair die - the big blue picture.) We used that one distribution,
| to "create" many (1000) distributions by sampling with replacement
| from the given one. We sampled 50000 times so we created 1000
| distributions of 50 rolls each.

We then calculated the mean of each of our created distributions and
| got a distribution of means. Sampling the one distribution many
| times gives us some variability in the resulting statistics we
| calculate. We can then calculate the standard error and confidence
| intervals associated with the statistic.

...

  |===============                                              |  25%
| Before we go on to more theory, here's another example in which
| we'll try to find a distribution of medians of a population. Do you
| recall what a median is?

1: 50th percentile
2: the most frequent outcome
3: a person who talks to spirits
4: a point halfway between rare and well-done

Selection: 1

| Your dedication is inspiring!

  |================                                             |  26%
| Recall the father and son height data. Once again, we've loaded it
| for you. We've placed the height of the sons in the vector sh and
| the length of this vector is stored in the variable nh. Use the R
| command head to look at the first few entries of sh.




Now we'll create 1000 distributions of the same length as the
| original sh. We'll do this by sampling sh with replacement 1000*nh
| times and store the results in an array with 1000 rows, each with nh
| entries. Then we'll take the median of each row and plot the result.

...

  |===================                                          |  31%
| Note that every time we draw from the empirical distribution sh,
| each of its nh data points is equally likely to be pulled, therefore
| the probability of drawing any one is 1/nh. The 1000 samples we
| create will vary from the original.


 Here's the resulting density curve. This estimates the distribution
| of medians. The thick vertical line shows where the median of the
| original, observed data sh lies.

 We stored the 1000 medians of the resampled sets in the vector
| resampledMedians. Use the R function median to compute the median of
| numbers in this vector.

> median(resampledMedians)
[1] 68.61273

| You nailed it! Good job!

  |=====================                                        |  35%
| Now compute the median of the original sample sh.

> median(sh)
[1] 68.61582

| You are quite good my friend!

  |======================                                       |  36%
| Pretty close, right? Now back to theory. Suppose you have a
| statistic that estimates some population parameter, but you don't
| know its sampling distribution. The bootstrap principle uses the
| distribution defined by the observed data to approximate the
| sampling distribution of that statistic.

The nice thing about bootstrapping is that you can always do it with
| simulation. The general procedure follows by first simulating B
| complete data sets from the observed data by sampling with
| replacement. Make sure B is large and that you're sampling WITH
| replacement to create data sets the same size as the original.

| This approximates drawing from the sampling distribution of that
| statistic, at least as well as the data approximates the true
| population distribution. By calculating the statistic for each
| simulated data set and using these simulated statistics we can
| either define a confidence interval (e.g. find the 2.5 and the 97.5
| percentiles) or take the standard deviation to estimate a standard
| error of that statistic.


Notice that this process doesn't use any fancy math or asymptotics.
| The only assumption behind it is that the observed sample is
| representative of the underlying population.

 We've created the vector fh for you which contains the fathers'
| heights from the father son data we've been working with. It's the
| same length as the sons' data (1078) which is stored in nh. B, the
| number of bootstraps we want has been set to 1000. We'll do an
| example now in small steps.

...

  |==========================                                   |  43%
| Our one sample of observed data is in the vector fh. Use the R
| function sample to sample fh nh*B times. Set the argument replace to
| TRUE. Put the result in the variable sam.

sam <- sample(fh, nh * B, replace = TRUE)

| You're the best!

  |===========================                                  |  44%
| Now form sam into a matrix with B rows and nh columns. Use the R
| function matrix and put the result in resam

resam <- matrix(sam, B, nh)

| That's a job well done!

  |============================                                 |  46%
| Now use the R function apply to take the median (third argument) of
| each row of resam (first argument). Put the result in meds. The
| second argument, the number 1, specifies that the application of the
| function is to the rows of the first argument.


meds <- apply(resam, 1, median)

| You got it!

  |=============================                                |  47%
| Now look at the difference between the median of fh and the median
| of meds.

median(fh) - median(meds)
[1] 0.004105

| You're the best!

  |==============================                               |  49%
| Pretty close, right? Now use the R function sd to estimate the
| standard error of the vector meds.


sd(meds)
[1] 0.1041906

| You are doing so well!

  |==============================                               |  50%
| We previously did this same process for the sons' data and stored
| the resampled medians in the 1000-long vector resampledMedians. Find
| the standard error of resampledMedians.


sd(resampledMedians)
[1] 0.08297627

| You are quite good my friend!

  |===============================                              |  51%
| Now we'll find a 95% confidence interval for the sons' data with the
| R function quantile. The first argument is the vector of
| resampledMedians and the second is the expression c(.025,.975). Do
| this now.

> quantile(resampledMedians, c(.025, .975))
    2.5%    97.5% 
68.43733 68.80718 

| You are quite good my friend!

  |================================                             |  53%
| Pretty close quantiles, right? Now do the same thing for the
| fathers' data. Recall that it's stored in the vector meds.

quantile(meds, c(.025, .975))
    2.5%    97.5% 
67.54999 67.94110 

| Keep working like that and you'll get there!

  |=================================                            |  54%
| Another pair of close quantiles, but notice that these quantiles of
| the fathers' medians differ from those of the sons.

 Bootstrapping is a very diverse and complicated topic and we just
| skimmed the surface here. The technique we showed you is
| nonparametric, that is, it's not based on any parameterized family
| of probability distributions. We used only one set of observations
| that we assumed to be representative of the population.

Finally, the confidence intervals we calculated might not perform
| very well because of biases but the R package bootstrap provides an
| easy fix for this problem.


Now, to permutation testing, another handy tool used in group
| comparisons. As bootstrapping did, permutation testing samples a
| single dataset a zillion times and calculates a statistic based on
| these samplings.


Permutation testing, however, is based on the idea of exchangability
| of group labels. It measures whether or not outcomes are independent
| of group identity. Our zillion samples simply permute group labels
| associated with outcomes. We'll see an example of this.

...

  |=====================================                        |  61%
| Here's a picture from the dataset InsectSprays which contains counts
| of the number of bugs killed by six different sprays.


We'll use permutation testing to compare Spray B with Spray C.


Use the R command dim to find the dimensions of InsectSprays.

> dim(InsectSprays)
[1] 72  2

| All that practice is paying off!

  |========================================                     |  65%
| Now use the R command names to find what the two columns of
| InsectSprays contain.

names(InsectSprays)
[1] "count" "spray"

| Keep up the great work!

  |=========================================                    |  67%
| We'll use permutation testing to compare Spray B with Spray C. We
| subsetted data for these two sprays into a data frame subdata.
| Moreover, the two data frames Bdata and Cdata contain the data for
| their respective sprays.



Now use the R command range on Bdata$count to find the minimum and
| maximum counts for Spray B.

> range(Bdata$count)
[1]  7 21

| Your dedication is inspiring!

  |==========================================                   |  69%
| The picture makes more sense now, right? Now do the same for Spray
| C. Its data is in Cdata.

 range(Cdata$count)
[1] 0 7

| You are doing so well!

  |===========================================                  |  71%
| From the ranges (as well as the picture), the sprays look a lot
| different. We'll test the (obviously false) null hypothesis that
| their means are the same.

To make the analysis easier we've defined two arrays for you, one
| holding the counts for sprays B and C. It's call BCcounts. Look at
| it now.

> BCcounts
 [1] 11 17 21 11 16 14 17 17 19 21  7 13  0  1  7  2  3  1  2  1  3  0
[23]  1  4

| You nailed it! Good job!

  |=============================================                |  74%
| The second array we've defined holds the spray identification and
| it's called group. These two arrays line up with each other, that
| is, the first 12 entries of counts are associated with spray B and
| the last 12 with spray C.  Look at group now.


group
 [1] "B" "B" "B" "B" "B" "B" "B" "B" "B" "B" "B" "B" "C" "C" "C" "C"
[17] "C" "C" "C" "C" "C" "C" "C" "C"

| Your dedication is inspiring!

  |==============================================               |  75%
| We've also defined for you a one-line function testStat which takes
| two parameters, an array of counts and an array of associated
| identifiers. It assumes all the counts come from group B or group C.
| It subtracts the mean of the counts from group C from the mean of
| the counts of group B. Type testStat with no parentheses and no
| arguments to see how it's defined.

 testStat
function(w, g) mean(w[g == "B"]) - mean(w[g == "C"])
<environment: 0x7fd18b5ad038>

| Your dedication is inspiring!

  |===============================================              |  76%
| Now set a variable obs by invoking testStat with the arguments
| BCcounts and group and assigning the result to obs.

obs <- testStat(BCcounts, group)

| Keep up the great work!

  |===============================================              |  78%
| Take a peek at obs now.

> obs
[1] 13.25

| You are amazing!

  |================================================             |  79%
| Pretty big difference, right? You can check this by using mean on
| Bdata$count and on Cdata$count and subtracting the latter from the
| former. Equivalently, you can just apply mean to
| Bdata$count-Cdata$count. Do either one now.

mean(Bdata$count - Cdata$count)
[1] 13.25

| All that hard work is paying off!

  |=================================================            |  81%
| So, mean(Bdata$count)-mean(Cdata$count) equals
| mean(Bdata$count-Cdata$count) because ?

1: the data is special
2: mean is linear
3: mathemagic

Selection: 2

| Excellent job!

  |==================================================           |  82%
| Now this is where the permutation testing starts to involve
| resampling. We're going to test whether or not the particular group
| association of the counts affects the difference of the means.

 |===================================================          |  83%
| We'll keep the same array of counts, just randomly relabel them, by
| permuting the group array. R makes this process very easy. Calling
| the function sample (which we've used several times in this lesson)
| with one argument, an array name, will simply permute the elements
| of that array.

Call sample now on the array group to see what happens.

> sample(group)
 [1] "B" "C" "C" "B" "B" "C" "B" "C" "C" "B" "C" "B" "B" "C" "C" "B"
[17] "B" "B" "C" "B" "C" "C" "B" "C"

| You are quite good my friend!

  |=====================================================        |  86%
| The labels are all mixed up now. We'll do this permuting of labels
| and then we'll recalculate the difference of the means of the two
| "new" (really newly labelled) groups.

 We'll relabel and calculate the difference of means 10000 times and
| store the differences (of means) in the array perms. Here's what the
| code looks like perms <- sapply(1 : 10000, function(i)
| testStat(BCcounts, sample(group))). Try it now.

> perms <- sapply(1:10000, function(i) testStat(BCCounts, sample(group)))
 Error in mean(w[g == "B"]) : object 'BCCounts' not found 
> perms <- sapply(1:10000, function(i) testStat(BCcounts, sample(group)))

| You got it right!

  |======================================================       |  89%
| We can take the mean of the virtual array of the boolean expression
| perms > obs. Do this now.


mean(perms>obs)
[1] 0

| That's correct!

  |=======================================================      |  90%
| So on average 0 of the permutations had a difference greater than
| the observed. That means we would reject the null hypothesis that
| the means of the two sprays were equal.


Here's a histogram of the difference of the means. Looks pretty
| normal, right? We can see that the distribution runs roughly between
| -10 and +10 and it's centered around 0. The vertical line shows
| where the observed difference of means was and we see that it's
| pretty far away from the distribution of the resampled permutations.
| This means that group identification did matter and sprays B and C
| were quite different.

Here's the picture of the InsectSprays again. Suppose we run the
| same experiment, this time comparing sprays D and E, which look more
| alike. We've redefined testStat to look at these sprays and subtract
| the mean of spray E from the mean of spray D.

|==========================================================   |  94%
| We've also stored off the D and E data in DEcounts and the group
| labels in group. Run testStat now with DEcounts and group.

> testStat(DEcounts, group)
[1] 1.416667

| You are amazing!

  |==========================================================   |  96%
| We've stored off this value, 1.416667, in the variable obs for you.
| Now run the permutation command, with DEcounts. Here it is, perms <-
| sapply(1 : 10000, function(i) testStat(DEcounts, sample(group)))

> perms <- sapply(1:10000, function(i) testStat(DEcounts, sample(group)))

| You nailed it! Good job!

  |===========================================================  |  97%
| Finally, we can plot the histogram of the distribution of the
| difference of the means. We see that with these sprays the observed
| difference of means (the vertical line) is closer to the mean of the
| permuted labels. This indicates that sprays D and E are quite
| similar and we fail to reject the null hypothesis that the means
| were equal.


Congrats! We hope you weren't bugged too much by this lesson and
| feel like you've pulled yourself up by your bootstraps.










