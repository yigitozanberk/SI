---
title: "Week 4 Classnotes"
author: "Yigit Ozan Berk"
date: "6/21/2019"
output: html_document
---

# Power = 1 - Type II error rate

We've talked about a Type I error, rejecting the null hypothesis when it's true. We've structured our hypothesis test so that the probability of this happening is small. The other kind of error we could make is to fail to reject when the alternative is true (Type II error). Or we might think about the probability of rejecting the null when it is false. This is called Power = 1 - Type II error. We don't have as much control over this probability, since we've spent all of our flexibility guaranteeing that the Type I error rate is small.

One avenue for the control of power is at the design phase. There, assuming our finances let us, we can pick a large enough sample size so that we'd be likely to reject if the alternative is true. Thus the most frequent use of power is to help us design studies.


# Contents

- Power
- Multipe Comparisons
- Resampling
- Quiz
- Course Project
- Swirl Excercises

# Power

power is the probability of rejecting the null hypothesis when it is false.

```{r}
z <- qnorm(1 - alpha)
pnorm(mu0 + z * sigma/sqrt(n), mean = mua, sd = sigma/sqrt(n), lower.tail = FALSE)
```


As you collect more and more sample for data, the power increases faster.

```{r}
library(manipulate)
library(ggplot2)
mu0 <- 30
#alpha is Type I error rate
myplot <- function(sigma, mua, n, alpha) {
        g <- ggplot(data.frame(mu = c(27, 36)), aes(x = mu))
        g <- g + stat_function(fun= dnorm, geom = "line",
                               args = list(mean = mu0, sd = sigma/sqrt(n)),
                               size = 2, col = "red")
        g <- g + stat_function(fun = dnorm, geom  = "line",
                               args = list(mean = mua, sd = sigma/sqrt(n)),
                               size = 2, col = "blue")
        xitc = mu0 + qnorm(1- alpha)*sigma/sqrt(n)
        g <- g + geom_vline(xintercept = xitc, size = 3)
        g
}

manipulate(
        myplot(sigma, mua, n, alpha),
        sigma = slider(1, 10, step = 1, initial = 4),
        mua = slider(30, 35, step = 1, initial = 32),
        n = slider(1, 50, step = 1, initial = 16),
        alpha = slider(0.01, 0.1, step = 0.01, initial = 0.05)
)

```

- power goes up as alpha gets larger
- power of a one sided test is greater than the power of the associated two sided test(alpha/2)
- power goes up as mu1 gets further away from mu0
- power goes up as n goes up

# T-test power

delta equals mua - mu0
delta = mua - mu0

```{r}
power.t.test(n = 16, delta = 2/4, sd = 1, type = "one.sample", alt = "one.sided")$power

```

```{r}
power.t.test(n = 16, delta = 2, sd = 4, type = "one.sample", alt = "one.sided")$power
```

```{r}
power.t.test(n = 16, delta = 100, sd = 200, type = "one.sample", alt = "one.sided")$power
```

because we keep delta/sd same, the output is equivalent

let's try to calculate sample size

```{r}
power.t.test(power = 0.8, delta = 2/4, sd = 1, type = "one.sample", alt = "one.sided")$n
```

```{r}
power.t.test(power = 0.8, delta = 2, sd = 4, type = "one.sample", alt = "one.sided")$n
```

```{r}
power.t.test(power = 0.8, delta = 100, sd = 200, type = "one.sample", alt = "one.sided")$n
```


# Multiple Testing

when you're doing more than 1 hypothesis tests, you need to check your results for correction.

Hypothesis testing/significance analysis is commonly overused

correcting for multiple testing avoids false positives or discoveries

two key components:

- error measure
- correction


Error rates to check in a multiple testing environment :

- false positive rate : the rate at which false results (beta = 0) are called significant  E[V/m0]
- family wise error rate (FWER) : the probability of at least one false positive
Pr (V >= 1)
- false discovery rate(FDR) : the rate at which claims of significance are false E[V/R]
(meaning of letters in the screenshot of the table)


ANOTHER Approach:

Adjusted P-Values

- one approach is to adjust the threshold alpha
- a different aproach is to calculate "adjusted p-values"
- They are not p-values anymore!!!!
- but they can be used directly without adjusting alpha

```{r}
set.seed(1010093)
pValues <- rep(NA, 1000)
for(i in 1:1000) {
        y <- rnorm(20)
        x <- rnorm(20)
        pValues[i] <- summary(lm(y ~ x))$coeff[2,4]
        #2nd row 4th column is the p-value for the relationship between y and x
}

sum(pValues <0.05)
```

Even there is no connection between y and x, still we get 5 % of tests being performed are called significant

```{r}
#controls FWER
sum(p.adjust(pValues, method = "bonferroni") < 0.05)
```

```{r}
#Controls FDR
sum(p.adjust(pValues, method = "BH") <0.05)
#benjamini hawkbird correction for controlling FDR

```

Case study II : 50% true positives

```{r}
set.seed(1010093)
pValues <- rep(NA, 1000)

for(i in 1:1000){
        x <- rnorm(20)
        #first 500 beta = 0, last 500 beta = 2
        if(i <= 500){y <- rnorm(20)}else{y <- rnorm(20, mean= 2 * x)}
        pValues[i] <- summary(lm(y~x))$coeff[2, 4]
}
trueStatus <- rep(c("zero", "non zero"), each = 500)
table(pValues < 0.05, trueStatus)
```

There are %5 false positives

```{r}
# Controls FWER
table(p.adjust(pValues, method = "bonferroni") < 0.05, trueStatus)
```

we get rid of false positives but we also lose true positives.

```{r}
# Controls FDR
table(p.adjust(pValues, method = "BH") <0.05, trueStatus)
```

fewer false positives without losing true positives.


p-values versus adjusted p-values to understand what p adjustment does
```{r}
par(mfrow = c(1,2))
plot(pValues, p.adjust(pValues, method = "bonferroni"), pch = 19)
plot(pValues, p.adjust(pValues, method = "BH"), pch = 19)
```

