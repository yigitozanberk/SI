---
title: "Week 4 Classnotes"
author: "Yigit Ozan Berk"
date: "6/21/2019"
output: html_document
---

# Power = 1 - Type II error rate

We've talked about a Type I error, rejecting the null hypothesis when it's true. We've structured our hypothesis test so that the probability of this happening is small. The other kind of error we could make is to fail to reject when the alternative is true (Type II error). Or we might think about the probability of rejecting the null when it is false. This is called Power = 1 - Type II error. We don't have as much control over this probability, since we've spent all of our flexibility guaranteeing that the Type I error rate is small.

One avenue for the control of power is at the design phase. There, assuming our finances let us, we can pick a large enough sample size so that we'd be likely to reject if the alternative is true. Thus the most frequent use of power is to help us design studies.


# Contents

- Power
- Multipe Comparisons
- Resampling
- Quiz
- Course Project
- Swirl Excercises

# Power

power is the probability of rejecting the null hypothesis when it is false.

```{r}
z <- qnorm(1 - alpha)
pnorm(mu0 + z * sigma/sqrt(n), mean = mua, sd = sigma/sqrt(n), lower.tail = FALSE)
```


As you collect more and more sample for data, the power increases faster.

```{r}
library(manipulate)
library(ggplot2)
mu0 <- 30
#alpha is Type I error rate
myplot <- function(sigma, mua, n, alpha) {
        g <- ggplot(data.frame(mu = c(27, 36)), aes(x = mu))
        g <- g + stat_function(fun= dnorm, geom = "line",
                               args = list(mean = mu0, sd = sigma/sqrt(n)),
                               size = 2, col = "red")
        g <- g + stat_function(fun = dnorm, geom  = "line",
                               args = list(mean = mua, sd = sigma/sqrt(n)),
                               size = 2, col = "blue")
        xitc = mu0 + qnorm(1- alpha)*sigma/sqrt(n)
        g <- g + geom_vline(xintercept = xitc, size = 3)
        g
}

manipulate(
        myplot(sigma, mua, n, alpha),
        sigma = slider(1, 10, step = 1, initial = 4),
        mua = slider(30, 35, step = 1, initial = 32),
        n = slider(1, 50, step = 1, initial = 16),
        alpha = slider(0.01, 0.1, step = 0.01, initial = 0.05)
)

```

- power goes up as alpha gets larger
- power of a one sided test is greater than the power of the associated two sided test(alpha/2)
- power goes up as mu1 gets further away from mu0
- power goes up as n goes up

# T-test power

delta equals mua - mu0
delta = mua - mu0

```{r}
power.t.test(n = 16, delta = 2/4, sd = 1, type = "one.sample", alt = "one.sided")$power

```

```{r}
power.t.test(n = 16, delta = 2, sd = 4, type = "one.sample", alt = "one.sided")$power
```

```{r}
power.t.test(n = 16, delta = 100, sd = 200, type = "one.sample", alt = "one.sided")$power
```

because we keep delta/sd same, the output is equivalent

let's try to calculate sample size

```{r}
power.t.test(power = 0.8, delta = 2/4, sd = 1, type = "one.sample", alt = "one.sided")$n
```

```{r}
power.t.test(power = 0.8, delta = 2, sd = 4, type = "one.sample", alt = "one.sided")$n
```

```{r}
power.t.test(power = 0.8, delta = 100, sd = 200, type = "one.sample", alt = "one.sided")$n
```


# Multiple Testing

when you're doing more than 1 hypothesis tests, you need to check your results for correction.

Hypothesis testing/significance analysis is commonly overused

correcting for multiple testing avoids false positives or discoveries

two key components:

- error measure
- correction


Error rates to check in a multiple testing environment :

- false positive rate : the rate at which false results (beta = 0) are called significant  E[V/m0]
- family wise error rate (FWER) : the probability of at least one false positive
Pr (V >= 1)
- false discovery rate(FDR) : the rate at which claims of significance are false E[V/R]
(meaning of letters in the screenshot of the table)


ANOTHER Approach:

Adjusted P-Values

- one approach is to adjust the threshold alpha
- a different aproach is to calculate "adjusted p-values"
- They are not p-values anymore!!!!
- but they can be used directly without adjusting alpha

```{r}
set.seed(1010093)
pValues <- rep(NA, 1000)
for(i in 1:1000) {
        y <- rnorm(20)
        x <- rnorm(20)
        pValues[i] <- summary(lm(y ~ x))$coeff[2,4]
        #2nd row 4th column is the p-value for the relationship between y and x
}

sum(pValues <0.05)
```

Even there is no connection between y and x, still we get 5 % of tests being performed are called significant

```{r}
#controls FWER
sum(p.adjust(pValues, method = "bonferroni") < 0.05)
```

```{r}
#Controls FDR
sum(p.adjust(pValues, method = "BH") <0.05)
#benjamini hawkbird correction for controlling FDR

```

Case study II : 50% true positives

```{r}
set.seed(1010093)
pValues <- rep(NA, 1000)

for(i in 1:1000){
        x <- rnorm(20)
        #first 500 beta = 0, last 500 beta = 2
        if(i <= 500){y <- rnorm(20)}else{y <- rnorm(20, mean= 2 * x)}
        pValues[i] <- summary(lm(y~x))$coeff[2, 4]
}
trueStatus <- rep(c("zero", "non zero"), each = 500)
table(pValues < 0.05, trueStatus)
```

There are %5 false positives

```{r}
# Controls FWER
table(p.adjust(pValues, method = "bonferroni") < 0.05, trueStatus)
```

we get rid of false positives but we also lose true positives.

```{r}
# Controls FDR
table(p.adjust(pValues, method = "BH") <0.05, trueStatus)
```

fewer false positives without losing true positives.


p-values versus adjusted p-values to understand what p adjustment does
```{r}
par(mfrow = c(1,2))
plot(pValues, p.adjust(pValues, method = "bonferroni"), pch = 19)
plot(pValues, p.adjust(pValues, method = "BH"), pch = 19)
```

# Resampling

- Bootstrapping
- Notes on bootsrapping
- permutation

a tremendously useful tool for constructing confidence intervals and calculating standard errors for difficult statistics

one of the most important procedures discovered. really liberated data analysts for inferences.

for example, how would one derive a confidence interval for the median?

# bootstrapping example

```{r}
library(UsingR)
data(father.son)
x <- father.son$sheight
n <- length(x)
B <- 10000
resamples <- matrix(sample(x, n * B, replace = TRUE), B, n)
# draw from x(sample()) use n * B numbers of resamples, where every time you take an observation you can replicate.
# arrange with Bootstrap rows, number of sample sizes columns
#arrange for B rows, n columns
# every row of this matrix is now a completely resampled data set  with original datas and sample size from the original data set.
resampleMedians <- apply(resamples, 1, median)
qplot(resampleMedians)
```

# Notes on Bootstrap

Principle : if you have a statistic that estimates some population parameter, but I don't know its sampling distribution

the bootstrap principle suggests using the distribution defined by the data to approximate sampling distribution

in practice, the bootstrap principle is always carried out using simulation

we will cover only a few aspects of bootstrap resampling

1 - sample n observations with replacement from the observed data resulting in one simulated complete data set.

2 - take the median of the simulated data set

3 - repeat these two steps B times, resulting in B simulated medians.(10000 or more simulations)

you want B to be large for the Monte Carlo error to be small.

4 - these medians are approximately drawn from the sampling distribution of the median of n observations; therefore we can
- draw a histogram of them
- calculate their standard deviation to estimate the standard error of the median
- take the 2.5% and 97.5 % percentiles as a confidence interval for the median

```{r}
B <- 10000
medians <- apply(resamples, 1, median)
sd(medians)
```

estimated standard error of the median

```{r}
quantile(medians, c(0.025, 0.975))
```

fairly tight confidence interval

```{r}
g <- ggplot(data.frame(medians = medians), aes(x = medians))
g <- g + geom_histogram(color = "black", fill = "lightblue", binwidth = 0.05)
g
```

this plot is an ESTIMATE of the sample distribution of the median

if we had the true population distribution, and sample it over and over again, this would be a good estimate with Monte Carlo error.


- bootstrap is non-parametric
- better percentile bootstrap confidence intervals correct for bias
- there are lots of variations on bootstrap procedures; the book "An introduction to the Bootstrap" by Efron and Tibshirani is a great place to start for both bootstrap and jackknife information

# permutation tests

used for group comparisons

- consider the null hypothesis that the distribution of the observations from each group is the same
- then, the group labels are irrelevant
- consider a data fram with count column and spray(group) label column
- permute the spray(group) labels column
- recalculate the statistics
mean difference in counts, geometric means, T statistic..
- calculate the percentage of simulations where the simulated statistic was more extreme (toward the alternative) than the observed


```{r}
subdata <- InsectSprays[InsectSprays$spray %in% c("B", "C"), ]
y <- subdata$count
group <- as.character(subdata$spray)
testStat <- function(w, g) mean(w[g == "B"]) - mean(w[g == "C"])
observedStat <- testStat(y, group)
#correct data
permutations <- sapply(1: 10000, function(i) testStat(y, sample(group)))
#sampled data
#under the null hypothesis that the group label is not related to outcome.
observedStat

```

```{r}
mean(permutations > observedStat)
```

in this data set we get 0. we couldn't find a configuration

the p value is very small, close to zero(scientificly, because the permutation form of our original data is at least as big as observedStat.). so we of course reject the null hypothesis.

```{r}
qplot(permutations) + geom_vline(xintercept = 13.25)
```

as the plot shows, the original difference in original data are on the far right, as we guessed before. we can now formally present a null hypothesis test.



