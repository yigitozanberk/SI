---
title: "Statistical Inference - Week 1"
author: "Yigit Ozan Berk"
date: "6/1/2019"
output: html_document
---

# Contents of the Course

1. introduction
2. probability
3.conditional probability
4.expectations
5.variance
6.common distributions
7.asymptotics
8.t confidence intervals
9. hypothesis testing
10. p-values
11.power
12. multiple testing
13. resampling

# Week 1 modules:

- introduction
- probability
- conditional probability
- expected values
- swirl exercises

# Probability mass functions/densities

densities and mass functions for random variables are the best starting point for ways to model and think about probabilities for numeric outcomes of experiments

random variables are :
- discrete
- or continuous

the web site traffic on a given day is a discrete random variable with no upper limit. Poisson distribution is a nice fit.

the BMI of a subject four years after a baseline measurement is a continuous random variable.

PMF:
- p must always be larger than or equal to 0
- the sum of all possibilities has to add up to 1.

p(x) = (1/2)^x (1/2)^(1-x) , X = 0, 1

p(x) = theta^x (1-theta)^(1-x) for X = 0, 1

PDF:
- it must be larger than or equal to zero everywhere
- total area under the curve must equal to 1.

f(x) = { 2x for 0<x<1
         0  otherwise }
         

```{r}
x <- c(-0.5, 0, 1, 1, 1.5)
y <- c(0, 0, 2, 0, 0)
plot(x, y, lwd = 3, frame = FALSE, type = "l")
```

it's always equal or bigger than 0. The total area under the curve equals 1. This is a valid pdf. 

-What is the prob. that 75 % or fewer of calls get addressed?

```{r}
#at point .75, the height is 1.5
#this is beta distribution.

pbeta(0.75, 2, 1)
```

CDF 
cumulative distribution function. 

F(x) = P(X <= x)

Survival function

S(x) = P(X > x)

```{r}
pbeta(c(0.4, 0.5, 0.6), 2, 1)
```

The probability of 40% or less calls get answered on a given day is 16%
The probability of 50% or less calls get answered on a given day is 25%
The probability of 60% or less calls get answered on a given day is 36%

F(x) = P(X<= x) = 1/2 base * height = 1/2(x)*(2x) = x^2

alpha'th quantile of a distribution  with distribution function F is the point xalpha so that:
F(xalpha) = alpha

To find the 50th quantile, 0.5 = F(x) = x^2

```{r}
sqrt(0.5)
```

So, on about 50 % of the days, about 70% of the phone calls or more get answered.

R can approximate quantiles for you for common distribution

```{r}
qbeta(0.5, 2, 1)
```
This is the median of the population.

2 and 1 are parameters for the specific beta density


# Conditional Probability

let B be an event so that P(B) > 0

P(A|B) = P(A n B) / P(B)

if A and B are independent

P(A|B) = P(A n B) / P(B) = P(A)P(B)/P(B) = P(A)

## Bayes' rule

when you want to find P(B|A) but you have P(A|B)

P(B|A) = P(A|B)P(B) / (P(A|B)P(B) + P(A|Bc)P(Bc))


Sensitivity = P(+|D)
Specificity = P(-|Dc)

+ for positive
- for negative test result

D for disease, Dc for no disease

Positive predictive value = P(D|+)
Negative predictive value = P(Dc|-)

## Likelihood ratios

probability/1-probability

odds of disease post-test

odds of the disease in the absence of the test result * diagnostic likelihood ratio of the test result

post-test odds of D = DLR+ * pre-test odds of D

specificity 98.5 %
sensitivity 99.7 %

DLR+ = .997/(1-.985) which is approx. 66

DLR- = (1-.997)/.985 which is approx. 0.003


IID means independent and identically distributed - for independent variables

# Expected Values

Expected values are properties of distributions

The mean is a characterization of the center
Variance and standard deviation are charactarizations of how the spread is


E[X] = sum(x*p(x))

E[X] represents the center of mass of a collection of locations and weights {x, p(x)}

the sample mean estimates this population mean.

```{r}
library(manipulate)
```

mean squared error is a measure of imbalance

*The purpose of Statistics is finding gravity points for the significant attributes of a population.*

*The values are never final, the attributes move around the neightbourhood of certain values within confidence interval certainties.*
*There is always room for error.*

The distribution of averages of a population will be centered at the same place as the distribution of the population itself.

## Summary

- Expected values are properties of distributions.
- The population mean is the center of mass of population.(infinite coin flips)
- The sample mean is the center of mass of the observed data.(mean of 10 coin flips)
- The sample mean is an estimate of the population mean.
- The sample mean is unbiased. The population mean of its distribution is the mean that it's trying to estimate.
- The more data that goes into the sample mean, the more concentrated its density/mass function is around the population mean.

# Swirld Practices

to download the swirl course:

```{r}
library(swirl)
install_from_swirl("Statistical Inference")
```

# Swirl W1
Introduction
probability1
probability2
conditional probability
expectations

We want to emphasize a couple of important points here. First, a statistic (singular) is a number
| computed from a sample of data. We use statistics to infer information about a population. Second,
| a random variable is an outcome from an experiment. Deterministic processes, such as computing
| means or variances, applied to random variables, produce additional random variables which have
| their own distributions. It's important to keep straight which distributions you're talking about.

...

  |=========================================================================                  |  80%
| Finally, there are two broad flavors of inference. The first is frequency, which uses "long run
| proportion of times an event occurs in independent, identically distributed repetitions." The
| second is Bayesian in which the probability estimate for a hypothesis is updated as additional
| evidence is acquired. Both flavors require an understanding of probability so that's what the next
| lessons will cover.

| If A and B are two independent events then the probability of them both occurring is the product
| of the probabilities. P(A&B) = P(A) * P(B)





| A probability density function is associated with a continuous random variable. To quote from
| Wikipedia, it "is a function that describes the relative likelihood for this random variable to
| take on a given value. The probability of the random variable falling within a particular range of
| values is given by ... the area under the density function but above the horizontal axis and
| between the lowest and greatest values of the range."


| The cumulative distribution function (CDF) of a random variable X, either discrete or continuous,
| is the function F(x) equal to the probability that X is less than or equal to x. In the example
| above, the area of the blue triangle represents the probability that the random variable was less
| than or equal to the value 1.6.

 mypdf
function(x){x/2}
<environment: 0x7fec4eb12f90>

| You got it right!

  |============================================================                               |  66%
| Now use the R function integrate to integrate mypdf with the parameters lower equal to 0 and upper
| equal to 1.6. See if you get the same area (probability) you got before.


integrate(mypdf, 0, 1.6)
0.64 with absolute error < 7.1e-15




| The survivor function S(x) of a random variable X is defined as the function of x equal to the
| probability that the random variable X is greater than the value x. This is the complement of the
| CDF F(x), in our example, the portion of the lower triangle that is not shaded.

...

  |=================================================================                          |  71%
| In our example, which of the following expressions represents the survival function?

1: 1-x*2x/2
2: 1-x^2
3: 1-x*x/4
4: 1-x*x/2

Selection: 3

| You are really on a roll!

  |====================================================================                       |  74%
| The quantile v of a CDF is the point x_v at which the CDF has the value v. More precisely,
| F(x_v)=v. A percentile is a quantile in which v is expressed as a percentage.

| What is the 50th percentile of the CDF F(x)=(x^2)/4 from the example above?

| Solve for the x such that x^2=4*.5=2

 sqrt(2)
[1] 1.414214


| A probability model connects data to a population using assumptions.

...

  |===================================================================================        |  91%
| Be careful to distinguish between population medians and sample medians.

| A sample median is an estimator of a population median (the estimand).





| We represent the conditional probability of an event A given that B has occurred with the notation
| P(A|B). More specifically, we define the conditional probability of event A, given that B has
| occurred with the following.

| P(A|B) = P(A & B)/ P(B) .
P(A|B) is the probability that BOTH A and B occur divided by the probability that B occurs.


| From the definition of P(A|B), we can write P(A&B) = P(A|B) * P(B), right?  Let's use this to
| express P(B|A).

| P(B|A) = P(B&A)/P(A) = P(A|B) * P(B)/P(A). This is a simple form of Bayes' Rule which relates the
| two conditional probabilities.



| Suppose we don't know P(A) itself, but only know its conditional probabilities, that is, the
| probability that it occurs if B occurs and the probability that it occurs if B doesn't occur.
| These are P(A|B) and P(A|~B), respectively. We use ~B to represent 'not B' or 'B complement'.

| We can then express P(A) = P(A|B) * P(B) + P(A|~B) * P(~B) and substitute this is into the
| denominator of Bayes' Formula.

| P(B|A) = P(A|B) * P(B) / ( P(A|B) * P(B) + P(A|~B) * P(~B) )


| Suppose we know the accuracy rates of the test for both the positive case (positive result when
| the patient has HIV) and negative (negative test result when the patient doesn't have HIV). These
| are referred to as test sensitivity and specificity, respectively.


| Let 'D' be the event that the patient has HIV, and let '+' indicate a positive test result and '-'
| a negative. What information do we know? Recall that we know the accuracy rates of the HIV test.


| Suppose a person gets a positive test result and comes from a population with a HIV prevalence
| rate of .001. We'd like to know the probability that he really has HIV. Which of the following
| represents this?

Prevalence is the probability of having the disease, also called the prior probability of having the disease.

| We can use the prevalence of HIV in the patient's population as the value for P(D). Note that
| since P(~D)=1-P(D) and P(+|~D) = 1-P(-|~D) we can calculate P(D|+). In other words, we know values
| for all the terms on the right side of the equation. Let's do it!

| Disease prevalence is .001. Test sensitivity (+ result with disease) is 99.7% and specificity (-
| result without disease) is 98.5%. First compute the numerator, P(+|D)*P(D). (This is also part of
| the denominator.)

.997 * .001
[1] 0.000997

| You nailed it! Good job!

  |==============================================                                             |  50%
| Now solve for the remainder of the denominator, P(+|~D)*P(~D).

(1-.985) * (1-.001)
[1] 0.014985


| Now put the pieces together to compute the probability that the patient has the disease given his
| positive test result, P(D|+). Plug your last two answers into the formula P(+|D) * P(D) / ( P(+|D)
| * P(D) + P(+|~D) * P(~D) ) to compute P(D|+).

(0.000997) / (0.000997 + 0.014985)
[1] 0.06238268


| So the patient has a 6% chance of having HIV given this positive test result. The expression
| P(D|+) is called the positive predictive value. Similarly, P(~D|-), is called the negative
| predictive value, the probability that a patient does not have the disease given a negative test
| result.


| The diagnostic likelihood ratio of a positive test, DLR_+, is the ratio of the two + conditional
| probabilities, one given the presence of disease and the other given the absence. Specifically,
| DLR_+ = P(+|D) / P(+|~D). Similarly, the DLR_- is defined as a ratio. Which of the following do
| you think represents the DLR_-?


| Recall that P(+|D) and P(-|~D), (test sensitivity and specificity respectively) are accuracy rates
| of a diagnostic test for the two possible results. They should be close to 1 because no one would
| take an inaccurate test, right? Since DLR_+ = P(+|D) / P(+|~D) we recognize the numerator as test
| sensitivity and the denominator as the complement of test specificity.


| Now recall that DLR_- = P(-|D) / P(-|~D). Here the numerator is the complement of sensitivity and
| the denominator is specificity. From the arithmetic and what you know about accuracy tests, do you
| expect DLR_- to be large or small?


| Now a little more about likelihood ratios. Recall Bayes Formula. P(D|+) = P(+|D) * P(D) / ( P(+|D)
| * P(D) + P(+|~D) * P(~D) ) and notice that if we replace all occurrences of 'D' with '~D', the
| denominator doesn't change. This means that if we formed a ratio of P(D|+) to P(~D|+) we'd get a
| much simpler expression (since the complicated denominators would cancel each other out). Like
| this....

...

  |===============================================================                            |  69%
| P(D|+) / P(~D|+) = P(+|D) * P(D) / (P(+|~D) * P(~D)) = P(+|D)/P(+|~D) * P(D)/P(~D).

| The left side of the equation represents the post-test odds of disease given a positive test
| result. The equation says that the post-test odds of disease equals the pre-test odds of disease
| (that is, P(D)/P(~D) ) times

2: the DLR_+


| In other words, a DLR_+ value equal to N indicates that the hypothesis of disease is N times more
| supported by the data than the hypothesis of no disease.

| Taking the formula above and replacing the '+' signs with '-' yields a formula with the DLR_-.
| Specifically, P(D|-) / P(~D|-) = P(-|D) / P(-|~D) * P(D)/P(~D). As with the positive case, this
| relates the odds of disease post-test, P(D|-) / P(~D|-), to those of disease pre-test, P(D)/P(~D).



| We'll conclude with iid. Random variables are said to be iid if they are independent and
| identically distributed. By independent we mean "statistically unrelated from one another".
| Identically distributed means that "all have been drawn from the same population distribution".



| The expected value of a random variable X, E(X), is a measure of its central tendency. For a
| discrete random variable X with PMF p(x), E(X) is defined as a sum, over all possible values x, of
| the quantity x*p(x). E(X) represents the center of mass of a collection of locations and weights,
| {x, p(x)}.


| Another term for expected value is mean. Recall your high school definition of arithmetic mean (or
| average) as the sum of a bunch of numbers divided by the number of numbers you added together.
| This is consistent with the formal definition of E(X) if all the numbers are equally weighted.



We've defined a function for you, expect_dice, which takes a PMF as an input. For our purposes,
| the PMF is a 6-long array of fractions. The i-th entry in the array represents the probability of
| i being the outcome of a dice roll. Look at the function expect_dice now.

> expect_dice
function(pmf){ mu <- 0; for (i in 1:6) mu <- mu + i*pmf[i]; mu}
<bytecode: 0x7fec588eac48>
<environment: 0x7fec584f3e18>

| All that practice is paying off!

  |=============                                                                              |  14%
| We've also defined PMFs for three dice, dice_fair, dice_high and dice_low. The last two are
| loaded, that is, not fair. Look at dice_high now.

> dice_high
[1] 0.04761905 0.09523810 0.14285714 0.19047619 0.23809524 0.28571429

| You are doing so well!

  |===============                                                                            |  16%
| Using the function expect_dice with dice_high as its argument, calculate the expected value of a
| roll of dice_high.

> expect_dice(dice_high)
[1] 4.333333

| Excellent job!

  |=================                                                                          |  19%
| See how the expected value of dice_high is higher than that of the fair dice. Now calculate the
| expected value of a roll of dice_low.

> expect_dice(dice_low)
[1] 2.666667

| That's the answer I was looking for.

  |===================                                                                        |  21%
| You can see the effect of loading the dice on the expectations of the rolls. For high-loaded dice
| the expected value of a roll (on average) is 4.33 and for low-loaded dice 2.67. We've stored these
| off for you in two variables, edh and edl. We'll need them later.

| One of the nice properties of the expected value operation is that it's linear. This means that,
| if c is a constant, then E(cX) = c*E(X). Also, if X and Y are two random variables then
| E(X+Y)=E(X)+E(Y). It follows that E(aX+bY)=aE(X)+bE(Y).

| For a continuous random variable X, the expected value is defined analogously as it was for the
| discrete case. Instead of summing over discrete values, however, the expectation integrates over a
| continuous function.

...

  |==============================                                                             |  33%
| It follows that for continuous random variables, E(X) is the area under the function t*f(t), where
| f(t) is the PDF (probability density function) of X. This definition borrows from the definition
| of center of mass of a continuous body.



| To find the expected value of this random variable you need to integrate the function t*f(t). Here
| f(t)=t/2, the diagonal line. (You might recall this from the last probability lesson.) The
| function you're integrating over is therefore t^2/2. We've defined a function myfunc for you
| representing this. You can use the R function 'integrate' with parameters myfunc, 0 (the lower
| bound), and 2 (the upper bound) to find the expected value. Do this now.

> integrate(myfunc, 0,2 )
1.333333 with absolute error < 1.5e-14

| You nailed it! Good job!

  |========================================                                                   |  44%
| As all the examples have shown, expected values of distributions are useful in characterizing
| them. The mean characterizes the central tendency of the distribution. However, often populations
| are too big to measure, so we have to sample them and then we have to use sample means. That's
| okay because sample expected values estimate the population versions. We'll show this first with a
| very simple toy and then with some simple equations.

| Look familiar? The result is the same as the mean of the original population spop. This is not
| because the example was specially cooked. It would work on any population. The expected value or
| mean of the sample mean is the population mean. What this means is that the sample mean is an
| unbiased estimator of the population mean.

...

  |=========================================================                                  |  63%
| Formally, an estimator e of some parameter v is unbiased if its expected value equals v, i.e.,
| E(e)=v. We can show that the expected value of a sample mean equals the population mean with some
| simple algebra.






