---
title: "Statistical Inference - Week 1"
author: "Yigit Ozan Berk"
date: "6/1/2019"
output: html_document
---

# Contents of the Course

1. introduction
2. probability
3.conditional probability
4.expectations
5.variance
6.common distributions
7.asymptotics
8.t confidence intervals
9. hypothesis testing
10. p-values
11.power
12. multiple testing
13. resampling

# Week 1 modules:

- introduction
- probability
- conditional probability
- expected values
- swirl exercises

# Probability mass functions/densities

densities and mass functions for random variables are the best starting point for ways to model and think about probabilities for numeric outcomes of experiments

random variables are :
- discrete
- or continuous

the web site traffic on a given day is a discrete random variable with no upper limit. Poisson distribution is a nice fit.

the BMI of a subject four years after a baseline measurement is a continuous random variable.

PMF:
- p must always be larger than or equal to 0
- the sum of all possibilities has to add up to 1.

p(x) = (1/2)^x (1/2)^(1-x) , X = 0, 1

p(x) = theta^x (1-theta)^(1-x) for X = 0, 1

PDF:
- it must be larger than or equal to zero everywhere
- total area under the curve must equal to 1.

f(x) = { 2x for 0<x<1
         0  otherwise }
         

```{r}
x <- c(-0.5, 0, 1, 1, 1.5)
y <- c(0, 0, 2, 0, 0)
plot(x, y, lwd = 3, frame = FALSE, type = "l")
```

it's always equal or bigger than 0. The total area under the curve equals 1. This is a valid pdf. 

-What is the prob. that 75 % or fewer of calls get addressed?

```{r}
#at point .75, the height is 1.5
#this is beta distribution.

pbeta(0.75, 2, 1)
```

CDF 
cumulative distribution function. 

F(x) = P(X <= x)

Survival function

S(x) = P(X > x)

```{r}
pbeta(c(0.4, 0.5, 0.6), 2, 1)
```

The probability of 40% or less calls get answered on a given day is 16%
The probability of 50% or less calls get answered on a given day is 25%
The probability of 60% or less calls get answered on a given day is 36%

F(x) = P(X<= x) = 1/2 base * height = 1/2(x)*(2x) = x^2

alpha'th quantile of a distribution  with distribution function F is the point xalpha so that:
F(xalpha) = alpha

To find the 50th quantile, 0.5 = F(x) = x^2

```{r}
sqrt(0.5)
```

So, on about 50 % of the days, about 70% of the phone calls or more get answered.

R can approximate quantiles for you for common distribution

```{r}
qbeta(0.5, 2, 1)
```
This is the median of the population.

2 and 1 are parameters for the specific beta density


# Conditional Probability

let B be an event so that P(B) > 0

P(A|B) = P(A n B) / P(B)

if A and B are independent

P(A|B) = P(A n B) / P(B) = P(A)P(B)/P(B) = P(A)

## Bayes' rule

when you want to find P(B|A) but you have P(A|B)

P(B|A) = P(A|B)P(B) / (P(A|B)P(B) + P(A|Bc)P(Bc))


Sensitivity = P(+|D)
Specificity = P(-|Dc)

+ for positive
- for negative test result

D for disease, Dc for no disease

Positive predictive value = P(D|+)
Negative predictive value = P(Dc|-)

## Likelihood ratios

probability/1-probability

odds of disease post-test

odds of the disease in the absence of the test result * diagnostic likelihood ratio of the test result

post-test odds of D = DLR+ * pre-test odds of D

specificity 98.5 %
sensitivity 99.7 %

DLR+ = .997/(1-.985) which is approx. 66

DLR- = (1-.997)/.985 which is approx. 0.003


IID means independent and identically distributed - for independent variables

# Expected Values

Expected values are properties of distributions

The mean is a characterization of the center
Variance and standard deviation are charactarizations of how the spread is


E[X] = sum(x*p(x))

E[X] represents the center of mass of a collection of locations and weights {x, p(x)}

the sample mean estimates this population mean.

```{r}
library(manipulate)
```

mean squared error is a measure of imbalance

*The purpose of Statistics is finding gravity points for the significant attributes of a population.*

*The values are never final, the attributes move around the neightbourhood of certain values within confidence interval certainties.*
*There is always room for error.*

The distribution of averages of a population will be centered at the same place as the distribution of the population itself.

## Summary

- Expected values are properties of distributions.
- The population mean is the center of mass of population.(infinite coin flips)
- The sample mean is the center of mass of the observed data.(mean of 10 coin flips)
- The sample mean is an estimate of the population mean.
- The sample mean is unbiased. The population mean of its distribution is the mean that it's trying to estimate.
- The more data that goes into the sample mean, the more concentrated its density/mass function is around the population mean.

# Swirld Practices

to download the swirl course:

```{r}
library(swirl)
install_from_swirl("Statistical Inference")
```














