---
title: 'Statistical Inference : Week 2 Classes'
author: "Yigit Ozan Berk"
date: "6/12/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Week 2

Variability
Distributions
Asymptotics
Excercises in Swirl
Quiz

# Variability

Var(X) = E[(X - mu)^2] = E[X^2] - E[X]^2

expected value of [x^2] minus the expected value of X quantity squared

square root of the variance is called standard deviation

E[X] = 3.5 - x is a toss of a die

E[X^2] = 1^2 * 1/6 + 2^2 * 1/6 + ....... + 6^2 * 1/6 = 15.17

Var(X) = E[X^2] - E[X]^2 equals approximately 2.92

the variance of a biased coin is p*(1-p)



population variance and the sample variance is directly proportional

Var(X) = E[(X - mu)^2] = E[X^2] - E[X]^2

Sample variance = S^2

sample variance is the average square of distance of the observed observations minus the sample mean.

the expected value of the sample variance is the population variance


# standard error

we call the standard deviation of a statistic 'a standard error'


#data example

```{r}
library(UsingR)
data(father.son)
x <- father.son$sheight
n <- length(x)
round(c(var(x), var(x)/n, sd(x), sd(x)/sqrt(n)), 2)
```

0.09 is the standard deviation in the distribution of averages of n children's heights

# Summary

the sample variance estimates the population variance

the distribution of the sample variance is centered at what it's estimating

it gets more concentrated around the population variance with larger sample sizes

the variance of the sample mean is the population variance devided by n

# Important distribution types

## Binomial distribution

'bernoulli distribution' - arises as the result of a binary outcome

only takes values 1 or 0 with probability p

variance is p*(1-p)

### binomial trials 

binomial random variable is the total number of heads in a potentially biased coin.

```{r}
choose(8,7) * .5^8 + choose(8,8) * .5^8

pbinom(6, size = 8, prob = .5, lower.tail = FALSE)
```


## normal distribution

gaussian distribution


```{r}
pnorm(1160, mean = 1020, sd = 50, lower.tail = F)

#is the same as

pnorm(2.8, lower.tail = FALSE)

#because (1160-1020)/50 equals 2.8
#1160 is 2.8 standard deviations away from the mean

#when calculating in terms of standard normals, that's x=2.8
#because sd = 1



```

## Poisson distribution

used to model counts

uses for poisson distribution

- modeling count data
- modeling event-time or survival data
- modeling contingency tables
- approximating binomials when n is large and p is small

```{r}
pbinom(2, size = 500, prob = 0.01)

ppois(2, lambda = 500 * 0.01)
```

# Asymptotics

when the sample size limits to infinity

asymptotics are incredibly useful for simple statistical inference and approximations

asymptotics from the bases for frequency interpretation of probabilities

these results allow us to talk about the large sample distribution of sample means of a collection of iid observations

```{r}
library(ggplot2)
n <- 1000
means <- cumsum(rnorm(n))/(1:n)

```

an estimator is consistent if it converges to what you want to estimate

# central limit theorem - CLT

for our purposes, the CLT states that the distribution of averages of iid variables (properly normalized) becomes that of a standard normal as the sample size increases

# asymptotics and the confidence intervals

```{r}
library(UsingR)
data(father.son)
x <- father.son$sheight
(mean(x)+ c(-1, 1)*qnorm(0.975) * sd(x)/sqrt(length(x)))/12 #in feet rather than inches


```

sample proportions of coin toss p

for %95 intervals, phat +- 1/sqrt(n) is a quick CI estimate for p

quick guidelines for estimation:

for 100 sample voters, 56 intent to vote for you.

1/sqrt(100)= 0.1 so a back of the envelope calculation gives and approximate 95% interval of
(0.46, 0.66)


```{r}
round(1/sqrt(10^(1:6)), 3)
```

```{r}
0.56 + c(-1, 1) * qnorm(0.975) * sqrt(0.56 * .44/100)
```

```{r}
binom.test(56, 100)$conf.int
```

```{r}
n <- 20
pvals <- seq(.1, .9, by = .05)
nosim <- 1000
coverage<- sapply(pvals, function(p) {
        phats <- rbinom(nosim, prob = p, size = n)/n
        ll <- phats - qnorm(0.975) * sqrt(phats * (1-phats)/n)
        ul <- phats + qnorm(0.975) * sqrt(phats * (1 - phats)/n)
        mean(ll < p & ul > p)
})

```

this gives bad coverage because n is small. it works fine when n > 100. try it..

quick fix

(X + 2)/( n + 4)

```{r}
n <- 20
pvals <- seq(.1, .9, by = .05)
nosim <- 1000
coverage<- sapply(pvals, function(p) {
        phats <- (rbinom(nosim, prob = p, size = n)+ 2)/(n+4)
        ll <- phats - qnorm(0.975) * sqrt(phats * (1-phats)/n)
        ul <- phats + qnorm(0.975) * sqrt(phats * (1 - phats)/n)
        mean(ll < p & ul > p)
})
```

# Swirl Ex. 

- variance
- commonDistros
- Asymptotics

# variance

| In this lesson, we'll discuss variances of distributions which, like means, are useful in
| characterizing them. While the mean characterizes the center of a distribution, the variance
| and its square root, the standard deviation, characterize the distribution's spread around
| the mean. As the sample mean estimates the population mean, so the sample variance estimates
| the population variance.

...

  |===                                                                    |   4%
| The variance of a random variable, as a measure of spread or dispersion, is, like a mean,
| defined as an expected value. It is the expected squared distance of the variable from its
| mean. Squaring the distance makes it positive so values less than and greater than the mean
| are treated the same. In mathematical terms, if X comes from a population with mean mu, then

...

  |====                                                                   |   6%
| Var(X) = E( (X-mu)^2 ) = E( (X-E(X))^2 ) = E(X^2)-E(X)^2

| Just as we distinguished between a population mean and a sample mean we have to distinguish
| between a population variance sigma^2 and a sample variance s^2. They are defined similarly
| but with a slight difference. The sample variance is defined as the sum of n squared
| distances from the sample mean divided by (n-1), where n is the number of samples or
| observations. We divide by n-1 because this is the number of degrees of freedom in the
| system. The first n-1 samples or observations are independent given the mean. The last one
| isn't independent since it can be calculated from the sample mean used in the formula.


| Now recall that the means of unbiased estimators equal the values they're trying to
| estimate. We can infer from the above that the sample variance is an unbiased estimator of
| population variance.


| Recall that the average of random samples from a population is itself a random variable with
| a distribution centered around the population mean. Specifically, E(X') = mu, where X'
| represents a sample mean and mu is the population mean.

| We can show that, if the population is infinite, the variance of the sample mean is the
| population variance divided by the sample size. Specifically, Var(X') = sigma^2 / n. Let's
| work through this in four short steps.

```{r}
# | Which of the following does Var(X') equal? Here X' represents the sample mean and 'Sum(X_i)'
# | represents the sum of the n samples X_1,...X_n. Assume these samples are independent.
# 
# 1: Var(1/n * Sum(X_i))
# 2: sigma
# 3: mu
# 4: E(1/n * Sum(X_i))
# 
# Selection: 1
# 
# | All that practice is paying off!
# 
#   |==================================                                     |  48%
# | Which of the following does Var(1/n * Sum(X_i)) equal?
# 
# 1: sigma/n
# 2: mu/n^2
# 3: 1/n^2*E(Sum(X_i))
# 4: 1/n^2*Var(Sum(X_i))
# 
# Selection: 4
# 
# | All that hard work is paying off!
# 
#   |====================================                                   |  50%
# | Recall that Var is an expected value and expected values are linear. Also recall that our
# | samples X_1, X_2,...,X_n are independent. What does Var(Sum(X_i)) equal?
# 
# 1: Var(sigma)
# 2: E(mu)
# 3: E(Sum(X_i))
# 4: Sum(Var(X_i))
# 
# Selection: 4
# 
# | All that practice is paying off!
# 
#   |=====================================                                  |  52%
# | Finally, each X_i comes from a population with variance sigma^2. What does Sum(Var(X_i))
# | equal? As before, Sum is taken over n values.
# 
# 1: n*mu
# 2: n*(sigma)^2
# 3: (n^2)*Var(sigma)
# 4: n*E(mu)
# 
# Selection: 2
# 
# | You got it right!
# 
#   |======================================                                 |  54%
# | So we've shown that
# | Var(X')=Var(1/n*Sum(X_i))=(1/n^2)*Var(Sum(X_i))=(1/n^2)*Sum(sigma^2)=sigma^2/n for infinite
# | populations when our samples are independent.
```


| The standard deviation of a statistic is called its standard error, so the standard error of
| the sample mean is the square root of its variance.


| Finally, here's something interesting. Chebyshev's inequality helps interpret variances. It
| states that the probability that a random variable X is at least k standard deviations from
| its mean is less than 1/(k^2). In other words, the probability that X is at least 2 standard
| deviations from the mean is less than 1/4, 3 standard deviations 1/9, 4 standard deviations
| 1/16, etc.

| However this estimate is quite conservative for random variables that are normally
| distributed, that is, with bell-curve distributions. In these cases, the probability of
| being at least 2 standard deviations from the mean is about 5% (as compared to Chebyshev's
| upper bound of 25%) and the probability of being at least 3 standard deviations from the
| mean is roughly .2%.


1: Exploratory Data Analysis
2: Getting and Cleaning Data
3: R Programming
4: Regression Models
5: Statistical Inference
6: Take me to the swirl course repository!

# CommonDistros

## Bernoulli
Given the same Bernoulli random variable above, which of the following represents E(X^2)

1: 1-p
2: (1-p)^2
3: p(1-p)
4: p^2
5: p

Selection: 4

| You almost had it, but not quite. Try again.

| Add the two terms x^2*p(x) where x equals 0 and 1 respectively.

1: p
2: 1-p
3: p^2
4: p(1-p)
5: (1-p)^2

| Binomial random variables are obtained as the sum of iid Bernoulli trials.  Specifically,
| let X_1, ..., X_n be iid Bernoulli(p) random variables; then X = X_1 + X_2 + ... X_n is a
| binomial random variable. Binomial random variables represent the number of successes, k,
| out of n independent Bernoulli trials. Each of the trials has probability p.


| You'll have to add together 3 terms each of the form, choose(5,x)*(.8)^x*(.2)^(5-x) for
| x=3,4,5 .

> choose(5, 3) * .8^3 * (.2)^(2) + choose(5, 4) * .8^4 * (.2)^(1) + choose(5, 5) * .8^5 * (.2)^(0)
[1] 0.94208

| Your dedication is inspiring!

  |=======================                                                              |  27%
| Now you can verify your answer with the R function pbinom. The quantile is 2, the size is 5,
| the prob is .8 and the lower.tail is FALSE. Try it now.

 pbinom(2, 5,prob = .8, lower.tail = FALSE)
[1] 0.94208

## normal dist

| Another very common distribution is the normal or Gaussian. It has a complicated density
| function involving its mean mu and variance sigma^2. The key fact of the density formula is
| that when plotted, it forms a bell shaped curve, symmetric about its mean mu. The variance
| sigma^2 corresponds to the width of the bell, the higher the variance, the fatter the bell.
| We denote a normally distributed random variable X as X ~ N(mu, sigma^2).

...

  |===========================                                                          |  32%
| When mu = 0 and sigma = 1 the resulting distribution is called the standard normal
| distribution and it is often labeled Z.


| Here are two useful facts concerning normal distributions. If X is a normal random variable
| with mean mu and variance sigma^2, i.e., X ~ N(mu,sigma^2),

...

  |============================================                                         |  52%
| then the random variable Z defined as Z = (X -mu)/sigma is normally distributed with mean 0
| and variance 1, i.e., Z ~ N(0, 1). (Z is standard normal.)

...

  |==============================================                                       |  55%
| The converse is also true. If Z is standard normal, i.e., Z ~ N(0,1), then the random
| variable X defined as X = mu + sigma*Z is normally distributed with mean mu and variance
| sigma^2, i.e., X ~ N(mu, sigma^2)

## poisson

| Now let's talk about our last common distribution, the Poisson. This is, as Wikipedia tells
| us, "a discrete probability distribution that expresses the probability of a given number of
| events occurring in a fixed interval of time and/or space if these events occur with a known
| average rate and independently of the time since the last event."


| The PMF of the Poisson distribution has one parameter, lambda. As with the other
| distributions the PMF calculates the probability that the Poisson distributed random
| variable X takes the value x. Specifically, P(X=x)=(lambda^x)e^(-lambda)/x!. Here x ranges
| from 0 to infinity.

...

  |======================================================================               |  82%
| The mean and variance of the Poisson distribution are both lambda.

...

  |=======================================================================              |  84%
| Poisson random variables are used to model rates such as the rate of hard drive failures. We
| write X~Poisson(lambda*t) where lambda is the expected count per unit of time and t is the
| total monitoring time.

| For example, suppose the number of people that show up at a bus stop is Poisson with a mean
| of 2.5 per hour, and we want to know the probability that at most 3 people show up in a 4
| hour period. We use the R function ppois which returns a probability that the random
| variable is less than or equal to 3. We only need to specify the quantile (3) and the mean
| (2.5 * 4). We can use the default parameters, lower.tail=TRUE and log.p=FALSE. Try it now.

> ppois(3, mean = 2.5*4, lower.tail= TRUE, log.p = FALSE)
Error in ppois(3, mean = 2.5 * 4, lower.tail = TRUE, log.p = FALSE) : 
  unused argument (mean = 2.5 * 4)
> ppois(3, lambda = 2.5*4, lower.tail= TRUE, log.p = FALSE)
[1] 0.01033605

| You nailed it! Good job!

  |===========================================================================          |  89%
| Finally, the Poisson distribution approximates the binomial distribution in certain cases.
| Recall that the binomial distribution is the discrete distribution of the number of
| successes, k, out of n independent binary trials, each with probability p. If n is large and
| p is small then the Poisson distribution with lambda equal to n*p is a good approximation to
| the binomial distribution.

## Asymptotics

| In this lesson, we'll discuss asymptotics, a topic which describes how statistics behave as
| sample sizes get very large and approach infinity. Pretending sample sizes and populations
| are infinite is useful for making statistical inferences and approximations since it often
| leads to a nice understanding of procedures.

 Asymptotics generally give no assurances about finite sample performance, but they form the
| basis for frequency interpretation of probabilities (the long run proportion of times an
| event occurs).

| Your output depends on R's random number generator, but your plot probably jumps around a
| bit and, by the 10th flip, your cumulative sum/10 is probably different from mine. If you
| did this several times, your plots would vary quite a bit. Now call coinPlot again, this
| time with 10000 as the argument.

> coinPlot(1000)

| Not quite right, but keep trying. Or, type info() for more options.

| Type coinPlot(10000) at the command prompt.

> coinPlot(10000)

| You are really on a roll!

  |=========                                                                            |  11%
| See the difference? Asymptotics in Action! The line approaches its asymptote of .5. This is
| the probability you expect since what we're plotting, the cumulative sum/number of flips,
| represents the probability of the coin landing on heads. As we know, this is .5 .

...

  |==========                                                                           |  12%
| We say that an estimator is CONSISTENT if it converges to what it's trying to estimate. The
| LLN says that the sample mean of iid samples is consistent for the population mean. This is
| good, right?

| Now for something really important - the CENTRAL LIMIT THEOREM (CLT) - one of the most
| important theorems in all of statistics. It states that the distribution of averages of iid
| variables (properly normalized) becomes that of a standard normal as the sample size
| increases.

 Let's dissect this to see what it means. First, 'properly normalized' means that you
| transformed the sample mean X'. You subtracted the population mean mu from it and divided
| the difference by sigma/sqrt(n). Here sigma is the standard deviation of the population and
| n is the sample size.

...

  |===============                                                                      |  18%
| Second, the CLT says that for large n, this normalized variable, (X'-mu)/(sigma/sqrt(n)) is
| almost normally distributed with mean 0 and variance 1. Remember that n must be large for
| the CLT to apply.

...

  |================                                                                     |  19%
| Do you recognize sigma/sqrt(n) from our lesson on variance? Since the population std
| deviation sigma is unknown, sigma/sqrt(n) is often approximated by what?

1: the mean of the population
2: the variance of the population
3: I give up
4: the standard error of the sample mean

Selection: 4

| Great job!

  |=================                                                                    |  21%
| Let's rephrase the CLT. Suppose X_1, X_2, ... X_n are independent, identically distributed
| random variables from an infinite population with mean mu and variance sigma^2. Then if n is
| large, the mean of the X's, call it X', is approximately normal with mean mu and variance
| sigma^2/n. We denote this as X'~N(mu,sigma^2/n).


| To show the CLT in action consider this figure from the slides. It presents 3 histograms of
| 1000 averages of dice rolls with sample sizes of 10, 20 and 30 respectively. Each average of
| n dice rolls (n=10,20,30) has been normalized by subtracting off the mean (3.5) then
| dividing by the standard error, sqrt(2.92/n). The normalization has made each histogram look
| like a standard normal, i.e., mean 0 and standard deviation 1.

...

  |====================                                                                 |  23%
| Notice that the CLT said nothing about the original population being normally distributed.
| That's precisely where its usefulness lies! We can assume normality of a sample mean no
| matter what kind of population we have, as long as our sample size is large enough and our
| samples are independent. Let's look at how it works with a binomial experiment like flipping
| a coin.

| Recall that if the probability of a head (call it 1) is p, then the probability of a tail
| (0) is 1-p. The expected value then is p and the variance is p-p^2 or p(1-p). Suppose we do
| n coin flips and let p' represent the average of these n flips. We normalize p' by
| subtracting the mean p and dividing by the std deviation sqrt(p(1-p)/n). Let's do this for
| 1000 trials and plot the resulting histogram.

...

  |======================                                                               |  26%
| Here's a figure from the slides showing the results of 3 such trials where each trial is for
| a different value of n (10, 20, and 30) and the coin is fair,so E(X)=.5 and the standard
| error is 1/(2sqrt(n)). Notice how with larger n (30) the histogram tightens up around the
| mean 0.

...

  |=======================                                                              |  27%
| Here's another plot from the slides of the same experiment, this time using a biassed coin.
| We set the probability of a head to .9, so E(X)=.9 and the standard error is sqrt(.09/n)
# !
| Again, the larger the sample size the more closely the distribution looks normal, although
| with this biassed coin the normal approximation isn't as good as it was with the fair coin.


| Note that for a 95% confidence interval we divide (100-95) by 2 (since we have both left and
| right tails) and add the result to 95 to compute the quantile we need. The 97.5 quantile is
| actually 1.96, but for simplicity it's often just rounded up to 2. If you wanted to find a
| 90% confidence interval what quantile would you want?

1: 85
2: 95
3: 100
4: 90

Selection: 
Enter an item from the menu, or 0 to exit
Selection: 4

| Not quite, but you're learning! Try again.

| Divide (100-90) by 2 and add this result to 90.

1: 95
2: 100
3: 90
4: 85

Selection: 1

| Keep up the great work!

  |===============================                                                      |  37%
| Use the R function qnorm to find the 95th quantile for a standard normal distribution.
| Remember that qnorm takes a probability as an input. You can use default values for all the
| other arguments.


| Another technique for calculating confidence intervals for binomial distributions is to
| replace p with p'. This is called the Wald confidence interval. We can also use the R
| function qnorm to get a more precise quantile value (closer to 1.96) instead of our ballpark
| estimate of 2.


| As an alternative to this Wald interval, we can also use the R function binom.test with the
| parameters 60 and 100 and let all the others default. This function "performs an exact test
| of a simple null hypothesis about the probability of success in a Bernoulli experiment."
| (This means it guarantees the coverages, uses a lot of computation and doesn't rely on the
| CLT.) This function returns a lot of information but all we want now are the values of the
| confidence interval that it returns. Use the R construct x$conf.int to find these now.

> binom.test(60, 100)

	Exact binomial test

data:  60 and 100
number of successes = 60, number of trials = 100, p-value = 0.05689
alternative hypothesis: true probability of success is not equal to 0.5
95 percent confidence interval:
 0.4972092 0.6967052
sample estimates:
probability of success 
                   0.6 


| Not quite! Try again. Or, type info() for more options.

| Type binom.test(60,100)$conf.int at the command prompt.

> binom.test(60, 100)$conf.int
[1] 0.4972092 0.6967052
attr(,"conf.level")
[1] 0.95

| That's correct!

  |============================================                                         |  52%
| Close to what we've seen before, right? Now we're going to see that the Wald interval isn't
| very accurate when n is small. We'll use the example from the slides.

| Suppose we flip a coin a small number of times, say 20. Also suppose we have a function
| mywald which takes a probability p, and generates 30 sets of 20 coin flips using that
| probability p. It uses the sampled proportion of success, p', for those 20 coin flips to
| compute the upper and lower bounds of the 95% Wald interval, that is, it computes the two
| numbers p'+/- qnorm(.975) * sqrt(p' * (1-p') / n) for each of the 30 trials. For the given
| true probability p, we count the number of times out of those 30 trials that the true
| probability p was in the Wald confidence interval. We'll call this the coverage.

...

  |===============================================                                      |  55%
| To make sure you understand what's going on, try running mywald now with the probability .2.
| It will print out 30 p' values (which you don't really need to see), followed by 30 lower
| bounds, 30 upper bounds and lastly the percentage of times that the input .2 was between the
| two bounds. See if you agree with the percentage you get. Usually it suffices to just count
| the number of times (out of the 30) that .2 is less than the upper bound.

| Now that you understand the underlying principle, suppose instead of 30 trials, we used 1000
| trials. Also suppose we did this experiment for a series of probabilities, say from .1 to .9
| taking steps of size .05. More specifically, we'll call our function using 17 different
| probabilities, namely .1, .15, .2, .25, ... .9 . We can then plot the percentages of
| coverage for each of the probabilities.


| Here's the plot of our results. Each of the 17 vertices show the percentage of coverage for
| a particular true probability p passed to the function. Results will vary, but usually the
| only probability that hits close to or above the 95% line is the p=.5 . So this shows that
| when n, the number of flips, is small (20) the CLT doesn't hold for many values of p, so the
| Wald interval doesn't work very well.


| Let's try the same experiment and increase n, the number of coin flips in each of our 1000
| trials, from 20 to 100 to see if the plot improves. Again, results may vary, but all the
| probabilities are much closer to the 95% line, so the CLT works better with a bigger value
| of n.

| A quick fix to the problem of having a small n is to use the Agresti/Coull interval. This
| simply means we add 2 successes and 2 failures to the counts when calculating the proportion
| p'. Specifically, if X is the number of successes out of the 20 coin flips, then instead of
| setting p'=X/20, let p'=(X+2)/24. We use 24 as the number of trials since we've added 2
| successes and 2 failures to the counts. Note that we still use 20 in the calculation of the
| upper and lower bounds.

Here's a plot using this Agresti/Coull interval, with 1000 trials of 20 coin flips each. It
| looks much better than both the original Wald with 20 coin flips and the improved Wald with
| 100 coin flips. However, this technique might make the confidence interval too wide.

| Why does this work? Adding 2 successes and 2 failures pulls p' closer to .5 which, as we
| saw, is the value which maximizes the confidence interval.

| To show this simply, we wrote a function ACCompar, which takes an integer input n. For each
| k from 1 to n it computes two fractions, k/n and (k+2)/(n+4). It then prints out the boolean
| vector of whether the new (k+2)/(n+4) fraction is less than the old k/n. It also prints out
| the total number of k's for which the condition is TRUE.

...

  |========================================================                             |  66%
| For all k less than n/2, you see FALSE indicating that the new fraction is greater than or
| equal to k/n. For all k greater than n/2 you see TRUE indicating that the new fraction is
| less than the old. If k=n/2 the old and new fractions are equal.

## Poisson
| Let's move on to Poisson distributions and confidence intervals. Recall that Poisson
| distributions apply to counts or rates. For the latter, we write X~Poisson(lambda*t) where
| lambda is the expected count per unit of time and t is the total monitoring time.




 Here's another example from the slides. Suppose a nuclear pump failed 5 times out of 94.32
| days and we want a 95% confidence interval for the failure rate per day. The number of
| failures X is Poisson distributed with parameter (lambda*t). We don't observe the failure
| rate, but we estimate it as x/t. Call our estimate lambda_hat, so lambda_hat=x/t. According
| to theory, the variance of our estimated failure rate is lambda/t. Again, we don't observe
| lambda, so we use our estimate of it instead. We thus approximate the variance of lambda_hat
| as lambda_hat/t.

...

  |=============================================================                        |  71%
| In this example what would you use as the estimated mean x/t?

1: I haven't a clue
2: 94.32/5
3: 5/94.32

Selection: 3

| That's a job well done!

  |==============================================================                       |  73%
| Set a variable lamb now with this value.

> lamb <- 5/94.32

| Excellent work!

  |===============================================================                      |  74%
| So lamb is our estimated mean and lamb/t is our estimated variance. The formula we've used
| to calculate a 95% confidence interval is est mean + c(-1,1)*qnorm(.975)*sqrt(est var). Use
| this formula now making the appropriate substitutions.

> lamb + c(-1,1)*qnorm(.975)*sqrt(lamb/94.32)
[1] 0.006545667 0.099476386

| Pretty close, right? Now to check the coverage of our estimate we'll run the same simulation
| experiment we ran before with binomial distributions. We'll vary our lambda values from .005
| to .1 with steps of .01 (so we have 10 of them), and for each one we'll generate 1000
| Poisson samples with mean lambda*t. We'll calculate sample means and use them to compute 95%
| confidence intervals. We'll then count how often out of the 1000 simulations the true mean
| (our lambda) was contained in the computed interval.


| Here's a plot of the results. We see that the coverage improves as lambda gets larger, and
| it's quite off for small lambda values.

| Now it's interesting to see how the coverage improves when we increase the unit of time. In
| the previous plot we used t=100 (rounding the 94.32 up). Here's a plot of the same
| experiment setting t=1000. We see that the coverage is much better for almost all the values
| of lambda, except for the smallest ones.



| Recall the size of the confidence interval positively depends on standard error which is
| sqrt(var/n). If variance is smaller then so is variability and the interval.

1: smaller
2: bigger

Selection: 1

| That's a job well done!

  |==================================================================================   |  96%
| If you had larger sample size would your confidence interval get bigger or smaller?

1: bigger
2: smaller

Selection: 2




